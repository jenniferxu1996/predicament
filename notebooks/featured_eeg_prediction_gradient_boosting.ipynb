{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44acde52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import re\n",
    "res_digit = r'[0-9]'\n",
    "\n",
    "# fourier transform\n",
    "from scipy.fft import fft, ifft\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "832db706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thisdir = /home/luke/git/external/predicament/notebooks\n",
      "Adding parent directory to python path\n",
      "sys.path =\n",
      "['/home/luke/git/external/predicament/notebooks', '/home/luke/git/external/predicament', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/home/luke/.local/lib/python3.10/site-packages', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/lib/python3.10/dist-packages']\n"
     ]
    }
   ],
   "source": [
    "# This is a hack to make the library in the parent folder available for imoprts\n",
    "# A better solution is by np8 here:\n",
    "# https://stackoverflow.com/questions/714063/importing-modules-from-parent-folder\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "thisdir = sys.path[0]\n",
    "print(f\"thisdir = {thisdir}\")\n",
    "parentdir = os.path.dirname(thisdir)\n",
    "#print(f\"parentdir = {parentdir}\")\n",
    "if not parentdir in sys.path:\n",
    "    print(\"Adding parent directory to python path\")\n",
    "    sys.path.insert(1, parentdir)\n",
    "else:\n",
    "    print(\"Skipping adding parent direct to path (there already)\")\n",
    "\n",
    "print(f\"sys.path =\\n{sys.path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28f17581",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ensure relative path to data directory is sound\n",
    "# for the notebook we need to modify the BASE_DATA_FOLDER\n",
    "import os \n",
    "os.environ['PREDICAMENT_DATA_DIR'] =  '../data'\n",
    "\n",
    "from predicament.utils.config import FEATURED_BASE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd724667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predicament.data.timeseries import create_participant_data_edf_only\n",
    "from predicament.data.windowed import window_all_participants_data\n",
    "from predicament.data.windowed import merge_condition_data\n",
    "from predicament.data.partitioning import between_subject_cv_partition\n",
    "\n",
    "from predicament.data.features import MAXIMAL_FEATURE_GROUP\n",
    "from predicament.data.features import STATS_FEATURE_GROUP\n",
    "from predicament.data.features import INFO_FEATURE_GROUP\n",
    "from predicament.data.features import FREQ_FEATURE_GROUP\n",
    "from predicament.data.features import convert_timeseries_to_features\n",
    "from prepare_evaluation_data import load_dataframe_and_config\n",
    "\n",
    "\n",
    "from predicament.data.datasets import propose_balanced_subject_condition_counts\n",
    "from predicament.data.datasets import subsample_proposed_subject_condition_counts\n",
    "from predicament.data.datasets import get_subject_condition_counts\n",
    "\n",
    "from predicament.evaluation.results import output_model_best_from_results\n",
    "from predicament.evaluation.results import save_results_df_to_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea485d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predicament.evaluation.results import test\n",
    "from predicament.evaluation.results import save_results_df_to_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015e1a17",
   "metadata": {},
   "source": [
    "## Load features\n",
    "\n",
    "Before running this, you will need to generate featured data. See README file for details. For the variable `subdir` below replace this with the subdirectory name of the featured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45e1ae95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fs: 250, n_samples = 1024, time: 4.096s, n_channels: 5\n"
     ]
    }
   ],
   "source": [
    "subdir = '20231129210920'\n",
    "featured_data_dir = os.path.join(FEATURED_BASE_PATH,subdir)\n",
    "\n",
    "featured_df, featured_config = load_dataframe_and_config(\n",
    "    featured_data_dir, 'featured.csv')\n",
    "n_channels = int(featured_config['LOAD']['n_channels'])\n",
    "channels = json.loads(featured_config['LOAD']['channels'].replace(\"'\",'\"'))\n",
    "participant_list = json.loads(featured_config['LOAD']['participant_list'].replace(\"'\",'\"'))\n",
    "Fs = int(featured_config['LOAD']['sample_rate'])\n",
    "window_size = int(featured_config['LOAD']['window_size'])\n",
    "time = window_size/Fs\n",
    "print(f\"Fs: {Fs}, n_samples = {window_size}, time: {time}s, n_channels: {n_channels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "408a74c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['part_ID', 'condition', 'start time', 'Mean0', 'Mean1', 'Mean2',\n",
       "       'Mean3', 'Mean4', 'SD0', 'SD1', 'SD2', 'SD3', 'SD4', 'MAD0', 'MAD1',\n",
       "       'MAD2', 'MAD3', 'MAD4', 'Max0', 'Max1', 'Max2', 'Max3', 'Max4', 'Min0',\n",
       "       'Min1', 'Min2', 'Min3', 'Min4', 'Energy0', 'Energy1', 'Energy2',\n",
       "       'Energy3', 'Energy4', 'IQR0', 'IQR1', 'IQR2', 'IQR3', 'IQR4',\n",
       "       'Correlation0', 'Correlation1', 'Correlation2', 'Correlation3',\n",
       "       'Correlation4', 'Correlation5', 'Correlation6', 'Correlation7',\n",
       "       'Correlation8', 'Correlation9', 'arCoeff0', 'arCoeff1', 'arCoeff2',\n",
       "       'arCoeff3', 'arCoeff4', 'arCoeff5', 'arCoeff6', 'arCoeff7', 'arCoeff8',\n",
       "       'arCoeff9', 'arCoeff10', 'arCoeff11', 'arCoeff12', 'arCoeff13',\n",
       "       'arCoeff14', 'arCoeff15', 'arCoeff16', 'arCoeff17', 'arCoeff18',\n",
       "       'arCoeff19', 'LyapunovExponent0', 'LyapunovExponent1',\n",
       "       'LyapunovExponent2', 'LyapunovExponent3', 'LyapunovExponent4',\n",
       "       'MaxFreqInd0', 'MaxFreqInd1', 'MaxFreqInd2', 'MaxFreqInd3',\n",
       "       'MaxFreqInd4', 'MeanFreq0', 'MeanFreq1', 'MeanFreq2', 'MeanFreq3',\n",
       "       'MeanFreq4', 'FreqSkewness0', 'FreqSkewness1', 'FreqSkewness2',\n",
       "       'FreqSkewness3', 'FreqSkewness4', 'FreqKurtosis0', 'FreqKurtosis1',\n",
       "       'FreqKurtosis2', 'FreqKurtosis3', 'FreqKurtosis4', 'SampleEntropy0',\n",
       "       'SampleEntropy1', 'SampleEntropy2', 'SampleEntropy3', 'SampleEntropy4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featured_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be838edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proposed_subject_condition_counts = [[304 322 461 283 296]\n",
      " [304 322 461 283 296]\n",
      " [304 110 344 283 296]\n",
      " [304 227 461 283 296]\n",
      " [304 322 344 227 296]\n",
      " [  0 110   0 283 296]\n",
      " [304 322   0 283 296]\n",
      " [304 322 461 283  86]\n",
      " [304 322   0 283 296]\n",
      " [304 322   0 283 296]\n",
      " [304 322 344 283 296]\n",
      " [304 322 461 283 296]]\n",
      "proposed_condition_counts = [3344 3345 3337 3340 3342]\n",
      "proposed_class_balance = [0.20014364 0.2002035  0.19972468 0.19990424 0.20002394]\n",
      "balancing featured data...\n",
      "after balancing: subject_condition_counts = [[304. 322. 461. 283. 296.]\n",
      " [304. 322. 461. 283. 296.]\n",
      " [304. 110. 344. 283. 296.]\n",
      " [304. 227. 461. 283. 296.]\n",
      " [304. 322. 344. 227. 296.]\n",
      " [  0. 110.   0. 283. 296.]\n",
      " [304. 322.   0. 283. 296.]\n",
      " [304. 322. 461. 283.  86.]\n",
      " [304. 322.   0. 283. 296.]\n",
      " [304. 322.   0. 283. 296.]\n",
      " [304. 322. 344. 283. 296.]\n",
      " [304. 322. 461. 283. 296.]]\n"
     ]
    }
   ],
   "source": [
    "# balancing the data\n",
    "proposed_subject_condition_counts = propose_balanced_subject_condition_counts(\n",
    "    featured_df)\n",
    "print(f\"proposed_subject_condition_counts = {proposed_subject_condition_counts}\")\n",
    "proposed_condition_counts = np.sum(proposed_subject_condition_counts, axis=0)\n",
    "print(f\"proposed_condition_counts = {proposed_condition_counts}\")\n",
    "proposed_class_balance = proposed_condition_counts/np.sum(proposed_condition_counts)\n",
    "print(f\"proposed_class_balance = {proposed_class_balance}\")\n",
    "print(f\"balancing featured data...\")\n",
    "featured_df = subsample_proposed_subject_condition_counts(\n",
    "    featured_df, proposed_subject_condition_counts)\n",
    "# post-check\n",
    "subject_condition_counts = get_subject_condition_counts(featured_df)\n",
    "print(f\"after balancing: subject_condition_counts = {subject_condition_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86e3223",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d06ccf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from skopt import BayesSearchCV\n",
    "# parameter ranges are specified by one of below\n",
    "from skopt.space import Real, Categorical, Integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d9fb7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns_to_use = ['Mean0', 'Mean1', 'Mean2', 'Mean3', 'Mean4', 'SD0', 'SD1', 'SD2', 'SD3', 'SD4', 'MAD0', 'MAD1', 'MAD2', 'MAD3', 'MAD4', 'Max0', 'Max1', 'Max2', 'Max3', 'Max4', 'Min0', 'Min1', 'Min2', 'Min3', 'Min4', 'Energy0', 'Energy1', 'Energy2', 'Energy3', 'Energy4', 'IQR0', 'IQR1', 'IQR2', 'IQR3', 'IQR4', 'Correlation0', 'Correlation1', 'Correlation2', 'Correlation3', 'Correlation4', 'Correlation5', 'Correlation6', 'Correlation7', 'Correlation8', 'Correlation9', 'arCoeff0', 'arCoeff1', 'arCoeff2', 'arCoeff3', 'arCoeff4', 'arCoeff5', 'arCoeff6', 'arCoeff7', 'arCoeff8', 'arCoeff9', 'arCoeff10', 'arCoeff11', 'arCoeff12', 'arCoeff13', 'arCoeff14', 'arCoeff15', 'arCoeff16', 'arCoeff17', 'arCoeff18', 'arCoeff19', 'MaxFreqInd0', 'MaxFreqInd1', 'MaxFreqInd2', 'MaxFreqInd3', 'MaxFreqInd4', 'MeanFreq0', 'MeanFreq1', 'MeanFreq2', 'MeanFreq3', 'MeanFreq4', 'FreqSkewness0', 'FreqSkewness1', 'FreqSkewness2', 'FreqSkewness3', 'FreqSkewness4', 'FreqKurtosis0', 'FreqKurtosis1', 'FreqKurtosis2', 'FreqKurtosis3', 'FreqKurtosis4']\n"
     ]
    }
   ],
   "source": [
    "# model name\n",
    "name = 'GradientBoosting'\n",
    "\n",
    "# choose which search to perform\n",
    "# search_type = 'random_search'\n",
    "search_type = 'bayesian_optimization'\n",
    "\n",
    "\n",
    "# you can choose a subset of the feature types to use\n",
    "features_to_use = set(\n",
    "    ['Mean', 'SD', 'MAD', 'Max', 'Min', 'Energy', 'IQR',\n",
    "     'arCoeff', 'Correlation', 'MaxFreqInd', 'MeanFreq', 'FreqSkewness',\n",
    "    'FreqKurtosis'\n",
    "    ])\n",
    "# this constructs a list of the featured data columns based on the above feature types\n",
    "columns_to_use = [ col for col in featured_df.columns if re.sub(res_digit, '', col) in features_to_use ]\n",
    "print(f\"columns_to_use = {columns_to_use}\")\n",
    "\n",
    "designmtx = featured_df[columns_to_use].to_numpy()\n",
    "\n",
    "condition_data = featured_df['condition'].values.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f34496d",
   "metadata": {},
   "source": [
    "## Hold one group out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5b39cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard cross-validation\n",
    "# Match all digits in the string and replace them with an empty string\n",
    "# new_string = re.sub(pattern, '', string1)\n",
    "subjects = np.unique(featured_df['part_ID'])\n",
    "n_subjects = len(subjects)\n",
    "groups = np.empty(len(featured_df), dtype=int)\n",
    "for s, sub in enumerate(subjects):\n",
    "    groups[featured_df['part_ID']==sub] = s\n",
    "# cross validation splits    \n",
    "group_kfold = GroupKFold(n_splits=n_subjects)\n",
    "# number of iterations for your search\n",
    "n_iter = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d15480",
   "metadata": {},
   "source": [
    "## Random Search\n",
    "\n",
    "Here we use a random grid to search for best hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15feda71",
   "metadata": {},
   "outputs": [],
   "source": [
    "if search_type == 'random_search':\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 10)]\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "    max_depth.append(None)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False] \n",
    "    # Create the random grid\n",
    "    random_grid = {\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_features': max_features,\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'min_samples_leaf': min_samples_leaf,\n",
    "        'bootstrap': bootstrap}\n",
    "\n",
    "    # First create the base model to tune\n",
    "    estimator = RandomForestClassifier()\n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    param_search = RandomizedSearchCV(\n",
    "        estimator = estimator,\n",
    "        param_distributions = random_grid,\n",
    "        n_iter = n_iter,\n",
    "        cv=group_kfold,\n",
    "        verbose=2,\n",
    "        random_state=42,\n",
    "        n_jobs = -1)\n",
    "\n",
    "    # Fit the random search model\n",
    "    _ = param_search.fit(X=designmtx, y=condition_data, groups=groups)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d210bd",
   "metadata": {},
   "source": [
    "## Bayesian Optimisation\n",
    "May be quicker and more effective than random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ff5afcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  12 | elapsed: 15.5min remaining:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed: 15.8min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  12 | elapsed: 68.4min remaining: 13.7min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed: 69.0min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  12 | elapsed:  2.4min remaining:   28.9s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:  2.5min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  12 | elapsed:  2.7min remaining:   32.9s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:  2.8min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  12 | elapsed:  6.2min remaining:  1.2min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m param_search \u001b[38;5;241m=\u001b[39m BayesSearchCV(\n\u001b[1;32m     24\u001b[0m     estimator, search_spaces,\n\u001b[1;32m     25\u001b[0m     cv\u001b[38;5;241m=\u001b[39mgroup_kfold, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_iter\u001b[38;5;241m=\u001b[39mn_iter,\n\u001b[1;32m     26\u001b[0m     n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# executes bayesian optimization\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mparam_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesignmtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcondition_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/skopt/searchcv.py:466\u001b[0m, in \u001b[0;36mBayesSearchCV.fit\u001b[0;34m(self, X, y, groups, callback, **fit_params)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_kwargs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_kwargs)\n\u001b[0;32m--> 466\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# BaseSearchCV never ranked train scores,\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# but apparently we used to ship this (back-compat)\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_train_score:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/sklearn/utils/validation.py:72\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPass \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m as keyword args. From version 0.25 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassing these as positional arguments will \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult in an error\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(args_msg)),\n\u001b[1;32m     70\u001b[0m                   \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m     71\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate({k: arg \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args)})\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/sklearn/model_selection/_search.py:736\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    732\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    733\u001b[0m             all_candidate_params, scorers, n_splits, all_out)\n\u001b[1;32m    734\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 736\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;66;03m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;66;03m# best_score_ iff refit is one of the scorer names\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;66;03m# In single metric evaluation, refit_metric is \"score\"\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultimetric_:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# If callable, refit is expected to return the index of the best\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# parameter set.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/skopt/searchcv.py:512\u001b[0m, in \u001b[0;36mBayesSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n_iter \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;66;03m# when n_iter < n_points points left for evaluation\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     n_points_adjusted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_iter, n_points)\n\u001b[0;32m--> 512\u001b[0m     optim_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_points_adjusted\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     n_iter \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m n_points\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_callbacks(callbacks, optim_result):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/skopt/searchcv.py:408\u001b[0m, in \u001b[0;36mBayesSearchCV._step\u001b[0;34m(self, search_space, optimizer, evaluate_candidates, n_points)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# make lists into dictionaries\u001b[39;00m\n\u001b[1;32m    406\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m [point_asdict(search_space, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params]\n\u001b[0;32m--> 408\u001b[0m all_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;66;03m# Feed the point and objective value back into optimizer\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# Optimizer minimizes objective, hence provide negative score\u001b[39;00m\n\u001b[1;32m    411\u001b[0m local_results \u001b[38;5;241m=\u001b[39m all_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_test_score\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(params):]\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/sklearn/model_selection/_search.py:708\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    705\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    706\u001b[0m               n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits))\n\u001b[0;32m--> 708\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m                                       \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    719\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    720\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/joblib/parallel.py:1061\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1061\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/joblib/parallel.py:940\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    939\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 940\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    942\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Bayesian optimisation\n",
    "\n",
    "if search_type == 'bayesian_optimization':\n",
    "\n",
    "    # this is just a naive initial guess at what will work\n",
    "    # note that the log-uniform prior helps to focus on \n",
    "    # smaller values\n",
    "    search_spaces = dict(\n",
    "        learning_rate = Real(1e-6, 5e-1, prior='log-uniform'),\n",
    "        n_estimators = Integer(10,200, prior='log-uniform'),\n",
    "        subsample = Real(1e-10, 1, prior='uniform'),\n",
    "#         criterion = Categorical(['friedman_mse', 'squared_error']),\n",
    "        #min_weight_fraction_leaf=0.0,\n",
    "        max_depth = Integer(1, 100,  prior='log-uniform'),\n",
    "        # min_impurity_decrease=0.0,\n",
    "        max_features = Real(1e-1, 1, prior='log-uniform'),\n",
    "        # max_leaf_nodes=None, \n",
    "        #ccp_alpha = \n",
    "    )\n",
    "\n",
    "    estimator = GradientBoostingClassifier()\n",
    "    # log-uniform: understand as search over p = exp(x) by varying x\n",
    "    param_search = BayesSearchCV(\n",
    "        estimator, search_spaces,\n",
    "        cv=group_kfold, verbose=3, random_state=42, n_iter=n_iter,\n",
    "        n_jobs = -1)\n",
    "\n",
    "    # executes bayesian optimization\n",
    "    _ = param_search.fit(X=designmtx, y=condition_data, groups=groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee08dd41",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BayesSearchCV' object has no attribute 'cv_results_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mparam_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv_results_\u001b[49m)\n\u001b[1;32m      2\u001b[0m result_df\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m, name)\n\u001b[1;32m      3\u001b[0m result_df\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheld out\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BayesSearchCV' object has no attribute 'cv_results_'"
     ]
    }
   ],
   "source": [
    "result_df = pd.DataFrame(param_search.cv_results_)\n",
    "result_df.insert(0, 'model', name)\n",
    "result_df.insert(1, 'held out', 'subject')\n",
    "result_df.insert(2, 'feature set', str(features_to_use))\n",
    "display(result_df)\n",
    "save_results_df_to_file(result_df, f'{search_type}_{name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e34752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_best_from_results(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefcb8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c34fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
