{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44acde52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import re\n",
    "res_digit = r'[0-9]'\n",
    "\n",
    "# fourier transform\n",
    "from scipy.fft import fft, ifft\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832db706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a hack to make the library in the parent folder available for imoprts\n",
    "# A better solution is by np8 here:\n",
    "# https://stackoverflow.com/questions/714063/importing-modules-from-parent-folder\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "thisdir = sys.path[0]\n",
    "print(f\"thisdir = {thisdir}\")\n",
    "parentdir = os.path.dirname(thisdir)\n",
    "#print(f\"parentdir = {parentdir}\")\n",
    "if not parentdir in sys.path:\n",
    "    print(\"Adding parent directory to python path\")\n",
    "    sys.path.insert(1, parentdir)\n",
    "else:\n",
    "    print(\"Skipping adding parent direct to path (there already)\")\n",
    "\n",
    "print(f\"sys.path =\\n{sys.path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f17581",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ensure relative path to data directory is sound\n",
    "# for the notebook we need to modify the BASE_DATA_FOLDER\n",
    "import os \n",
    "os.environ['PREDICAMENT_DATA_DIR'] =  '../data'\n",
    "\n",
    "from predicament.utils.config import FEATURED_BASE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd724667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predicament.data.timeseries import create_participant_data_edf_only\n",
    "from predicament.data.windowed import window_all_participants_data\n",
    "from predicament.data.windowed import merge_condition_data\n",
    "from predicament.data.partitioning import between_subject_cv_partition\n",
    "\n",
    "from predicament.data.features import MAXIMAL_FEATURE_GROUP\n",
    "from predicament.data.features import STATS_FEATURE_GROUP\n",
    "from predicament.data.features import INFO_FEATURE_GROUP\n",
    "from predicament.data.features import FREQ_FEATURE_GROUP\n",
    "from predicament.data.features import convert_timeseries_to_features\n",
    "from prepare_evaluation_data import load_dataframe_and_config\n",
    "\n",
    "\n",
    "from predicament.evaluation.balancing import get_group_label_counts\n",
    "from predicament.evaluation.balancing import balance_data\n",
    "\n",
    "from predicament.evaluation.results import output_model_best_from_results\n",
    "from predicament.evaluation.results import save_results_df_to_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea485d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predicament.evaluation.results import test\n",
    "from predicament.evaluation.results import save_results_df_to_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015e1a17",
   "metadata": {},
   "source": [
    "## Load features\n",
    "\n",
    "Before running this, you will need to generate featured data. See README file for details. For the variable `subdir` below replace this with the subdirectory name of the featured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e1ae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdir = '20231129210920'\n",
    "featured_data_dir = os.path.join(FEATURED_BASE_PATH,subdir)\n",
    "\n",
    "featured_df, featured_config = load_dataframe_and_config(\n",
    "    featured_data_dir, 'featured.csv')\n",
    "n_channels = int(featured_config['LOAD']['n_channels'])\n",
    "channels = json.loads(featured_config['LOAD']['channels'].replace(\"'\",'\"'))\n",
    "participant_list = json.loads(featured_config['LOAD']['participant_list'].replace(\"'\",'\"'))\n",
    "Fs = int(featured_config['LOAD']['sample_rate'])\n",
    "window_size = int(featured_config['LOAD']['window_size'])\n",
    "time = window_size/Fs\n",
    "print(f\"Fs: {Fs}, n_samples = {window_size}, time: {time}s, n_channels: {n_channels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a74c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be838edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance featured data\n",
    "subject_condition_counts = get_group_label_counts(featured_df, 'participant', 'condition')\n",
    "print(f\"before balancing: subject_condition_counts = {subject_condition_counts}\")\n",
    "featured_df = balance_data(featured_df, group_col='participant', label_col='condition')\n",
    "subject_condition_counts = get_group_label_counts(featured_df, 'participant', 'condition')\n",
    "\n",
    "print(f\"after balancing: subject_condition_counts = {subject_condition_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86e3223",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06ccf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from skopt import BayesSearchCV\n",
    "# parameter ranges are specified by one of below\n",
    "from skopt.space import Real, Categorical, Integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9fb7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model name\n",
    "name = 'GradientBoosting'\n",
    "\n",
    "# choose which search to perform\n",
    "# search_type = 'random_search'\n",
    "search_type = 'bayesian_optimization'\n",
    "\n",
    "\n",
    "# you can choose a subset of the feature types to use\n",
    "features_to_use = set(\n",
    "    ['Mean', 'SD', 'MAD', 'Max', 'Min', 'Energy', 'IQR',\n",
    "     'arCoeff', 'Correlation', 'MaxFreqInd', 'MeanFreq', 'FreqSkewness',\n",
    "    'FreqKurtosis'\n",
    "    ])\n",
    "# this constructs a list of the featured data columns based on the above feature types\n",
    "columns_to_use = [ col for col in featured_df.columns if re.sub(res_digit, '', col) in features_to_use ]\n",
    "print(f\"columns_to_use = {columns_to_use}\")\n",
    "\n",
    "designmtx = featured_df[columns_to_use].to_numpy()\n",
    "\n",
    "condition_data = featured_df['condition'].values.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f34496d",
   "metadata": {},
   "source": [
    "## Hold one group out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b39cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard cross-validation\n",
    "# Match all digits in the string and replace them with an empty string\n",
    "# new_string = re.sub(pattern, '', string1)\n",
    "subjects = np.unique(featured_df['participant'])\n",
    "n_subjects = len(subjects)\n",
    "groups = np.empty(len(featured_df), dtype=int)\n",
    "for s, sub in enumerate(subjects):\n",
    "    groups[featured_df['participant']==sub] = s\n",
    "# cross validation splits    \n",
    "group_kfold = GroupKFold(n_splits=n_subjects)\n",
    "# number of iterations for your search\n",
    "n_iter = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d15480",
   "metadata": {},
   "source": [
    "## Random Search\n",
    "\n",
    "Here we use a random grid to search for best hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15feda71",
   "metadata": {},
   "outputs": [],
   "source": [
    "if search_type == 'random_search':\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 10)]\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "    max_depth.append(None)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False] \n",
    "    # Create the random grid\n",
    "    random_grid = {\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_features': max_features,\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'min_samples_leaf': min_samples_leaf,\n",
    "        'bootstrap': bootstrap}\n",
    "\n",
    "    # First create the base model to tune\n",
    "    estimator = RandomForestClassifier()\n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    param_search = RandomizedSearchCV(\n",
    "        estimator = estimator,\n",
    "        param_distributions = random_grid,\n",
    "        n_iter = n_iter,\n",
    "        cv=group_kfold,\n",
    "        verbose=2,\n",
    "        random_state=42,\n",
    "        n_jobs = -1)\n",
    "\n",
    "    # Fit the random search model\n",
    "    _ = param_search.fit(X=designmtx, y=condition_data, groups=groups)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d210bd",
   "metadata": {},
   "source": [
    "## Bayesian Optimisation\n",
    "May be quicker and more effective than random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff5afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian optimisation\n",
    "\n",
    "if search_type == 'bayesian_optimization':\n",
    "\n",
    "    # this is just a naive initial guess at what will work\n",
    "    # note that the log-uniform prior helps to focus on \n",
    "    # smaller values\n",
    "    search_spaces = dict(\n",
    "        learning_rate = Real(1e-6, 5e-1, prior='log-uniform'),\n",
    "        n_estimators = Integer(10,200, prior='log-uniform'),\n",
    "        subsample = Real(1e-10, 1, prior='uniform'),\n",
    "#         criterion = Categorical(['friedman_mse', 'squared_error']),\n",
    "        #min_weight_fraction_leaf=0.0,\n",
    "        max_depth = Integer(1, 100,  prior='log-uniform'),\n",
    "        # min_impurity_decrease=0.0,\n",
    "        max_features = Real(1e-1, 1, prior='log-uniform'),\n",
    "        # max_leaf_nodes=None, \n",
    "        #ccp_alpha = \n",
    "    )\n",
    "\n",
    "    estimator = GradientBoostingClassifier()\n",
    "    # log-uniform: understand as search over p = exp(x) by varying x\n",
    "    param_search = BayesSearchCV(\n",
    "        estimator, search_spaces,\n",
    "        cv=group_kfold, verbose=3, random_state=42, n_iter=n_iter,\n",
    "        n_jobs = -1)\n",
    "\n",
    "    # executes bayesian optimization\n",
    "    _ = param_search.fit(X=designmtx, y=condition_data, groups=groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee08dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(param_search.cv_results_)\n",
    "result_df.insert(0, 'model', name)\n",
    "result_df.insert(1, 'held out', 'subject')\n",
    "result_df.insert(2, 'feature set', str(features_to_use))\n",
    "display(result_df)\n",
    "save_results_df_to_file(result_df, f'{search_type}_{name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e34752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_best_from_results(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefcb8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c34fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
