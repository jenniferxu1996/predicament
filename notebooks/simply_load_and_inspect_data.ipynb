{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ab621dc",
   "metadata": {},
   "source": [
    "This is a live notebook with experimental code to develop analysis investigating the correlation between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "518e95ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['debug_windowing_data.ipynb', 'testing_mlpwrapper.ipynb', 'logs', 'analyse_results.ipynb', 'gait_data_features.ipynb', 'feature_development.ipynb', 'data', 'feature_analysis.ipynb', 'test_sample_entropy_implementations.ipynb', 'merging_dataframes.ipynb', 'data_and_processing_description.ipynb', 'featured_prediction_random_forest.ipynb', 'window_timings_and_IBI.ipynb', 'gait_data_exploration.ipynb', 'processing_status.ipynb', '.ipynb_checkpoints', 'loading_e4_data.ipynb', 'data_investigation_scratch.ipynb', 'simply_load_and_inspect_data.ipynb', 'grouping_and_crossvalidation.ipynb', 'sample-entropy-numba-impl.ipynb', 'featured_prediction.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "import scipy.stats as spstats\n",
    "# fourier transform\n",
    "from scipy.fft import fft, ifft\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# from statsmodels.tsa.api import acf, graphics, pacf\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "# from statsmodels.tsa.ar_model import ar_select_order\n",
    "\n",
    "import os\n",
    "print(os.listdir(\".\"))\n",
    "\n",
    "\n",
    "import re\n",
    "res_digit = r'[0-9]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9652555c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thisdir = /home/luke/git/external/predicament/notebooks\n",
      "Adding parent directory to python path\n",
      "sys.path =\n",
      "['/home/luke/git/external/predicament/notebooks', '/home/luke/git/external/predicament', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/home/luke/.local/lib/python3.10/site-packages', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/lib/python3.10/dist-packages']\n"
     ]
    }
   ],
   "source": [
    "# This is a hack to make the library in the parent folder available for imoprts\n",
    "# A better solution is by np8 here:\n",
    "# https://stackoverflow.com/questions/714063/importing-modules-from-parent-folder\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "thisdir = sys.path[0]\n",
    "print(f\"thisdir = {thisdir}\")\n",
    "parentdir = os.path.dirname(thisdir)\n",
    "#print(f\"parentdir = {parentdir}\")\n",
    "if not parentdir in sys.path:\n",
    "    print(\"Adding parent directory to python path\")\n",
    "    sys.path.insert(1, parentdir)\n",
    "else:\n",
    "    print(\"Skipping adding parent direct to path (there already)\")\n",
    "\n",
    "print(f\"sys.path =\\n{sys.path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28f17581",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ensure relative path to data directory is sound\n",
    "# for the notebook we need to modify the BASE_DATA_FOLDER\n",
    "import os \n",
    "os.environ['PREDICAMENT_DATA_DIR'] =  '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd724667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predicament.utils.config import DREEM_EEG_CHANNELS\n",
    "from predicament.utils.config import FEATURED_BASE_PATH\n",
    "from predicament.utils.config import WINDOWED_BASE_PATH\n",
    "from predicament.utils.file_utils import drop_nan_cols\n",
    "from predicament.utils.file_utils import load_windowed_dataframe_and_config\n",
    "from predicament.utils.file_utils import load_dataframe_and_config\n",
    "from predicament.utils.file_utils import write_dataframe_and_config\n",
    "\n",
    "from predicament.data.timeseries import create_participant_data_edf_only\n",
    "from predicament.data.windowed import window_all_participants_data\n",
    "from predicament.data.windowed import merge_condition_data\n",
    "from predicament.data.partitioning import between_subject_cv_partition\n",
    "\n",
    "from predicament.data.features import IDEAL_FEATURE_GROUP\n",
    "from predicament.data.features import STATS_FEATURE_GROUP\n",
    "from predicament.data.features import INFO_FEATURE_GROUP\n",
    "from predicament.data.features import FREQ_FEATURE_GROUP\n",
    "from predicament.data.features import convert_timeseries_to_features\n",
    "from predicament.data.features import filter_features\n",
    "\n",
    "from prepare_evaluation_data import group_conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3596cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'VG_01': '../data/CARE_HOME_DATA/./VG01/E4_8921_15_44/',\n",
       " 'VG_03': '../data/CARE_HOME_DATA/./VG03/E4_9921_12_16/',\n",
       " 'VG_05': '../data/CARE_HOME_DATA/./VG05/E4_9921_13_24/',\n",
       " 'VG_06': '../data/CARE_HOME_DATA/./VG06/E4_51021_13_33/',\n",
       " 'VG_07': '../data/CARE_HOME_DATA/./VG07/E4_51021_15_39/',\n",
       " 'VG_08': '../data/CARE_HOME_DATA/./VG08/E4_71021_10_42/',\n",
       " 'VG_09': '../data/CARE_HOME_DATA/./VG09/E4_11221_14_46/',\n",
       " 'VG_10': '../data/CARE_HOME_DATA/./VG10/E4_31221_11_17/',\n",
       " 'VH_01': '../data/CARE_HOME_DATA/./VH01/E4_61021_11_03/',\n",
       " 'VH_02': '../data/CARE_HOME_DATA/./VH02/E4_61021_13_59/',\n",
       " 'VH_03': '../data/CARE_HOME_DATA/./VH03/E4_11221_11_22/'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from predicament.utils.config import E4_CSV_FILES\n",
    "from predicament.utils.config import E4_FULL_DIRPATHS\n",
    "E4_FULL_DIRPATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bdd7ed5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m subdir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_dreem_4secs\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m df, config \u001b[38;5;241m=\u001b[39m \u001b[43mload_windowed_dataframe_and_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubdir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m df\n",
      "File \u001b[0;32m~/git/external/predicament/predicament/utils/file_utils.py:117\u001b[0m, in \u001b[0;36mload_windowed_dataframe_and_config\u001b[0;34m(subdir, data_fname, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_windowed_dataframe_and_config\u001b[39m(subdir, data_fname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindowed.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     data_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(WINDOWED_BASE_PATH,subdir)\n\u001b[0;32m--> 117\u001b[0m     df, config \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataframe_and_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_fname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df, config\n",
      "File \u001b[0;32m~/git/external/predicament/predicament/utils/file_utils.py:142\u001b[0m, in \u001b[0;36mload_dataframe_and_config\u001b[0;34m(dir_path, data_fname, config_fname, **readargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m data_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dir_path, data_fname)\n\u001b[1;32m    141\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading dataframe from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 142\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mreadargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m config_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dir_path, config_fname)\n\u001b[1;32m    144\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading config from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers/readers.py:586\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    571\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    572\u001b[0m     dialect,\n\u001b[1;32m    573\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    583\u001b[0m )\n\u001b[1;32m    584\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers/readers.py:488\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers/readers.py:1047\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1046\u001b[0m     nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m-> 1047\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m col_dict:\n\u001b[1;32m   1051\u001b[0m             \u001b[38;5;66;03m# Any column is actually fine:\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers/c_parser_wrapper.py:224\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 224\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    226\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/_libs/parsers.pyx:801\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/_libs/parsers.pyx:880\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/_libs/parsers.pyx:1026\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/_libs/parsers.pyx:1073\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/_libs/parsers.pyx:1152\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/dtypes/common.py:722\u001b[0m, in \u001b[0;36mis_integer_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_integer_dtype\u001b[39m(arr_or_dtype) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;124;03m    Check whether the provided array or dtype is of an integer dtype.\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_is_dtype_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr_or_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses_and_not_datetimelike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minteger\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/dtypes/common.py:1596\u001b[0m, in \u001b[0;36m_is_dtype_type\u001b[0;34m(arr_or_dtype, condition)\u001b[0m\n\u001b[1;32m   1593\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m condition(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m   1595\u001b[0m \u001b[38;5;66;03m# fastpath\u001b[39;00m\n\u001b[0;32m-> 1596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marr_or_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m condition(arr_or_dtype\u001b[38;5;241m.\u001b[39mtype)\n\u001b[1;32m   1598\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr_or_dtype, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "subdir = 'binary_dreem_4secs'\n",
    "df, config = load_windowed_dataframe_and_config(subdir)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca380aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b4dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['condition'].unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9350c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_ = \"inactive:baseline,break;active:exper_video,wildlife_video,familiar_music,tchaikovsky,family_inter\"\n",
    "dict_ = get_dict_from_string(string_)\n",
    "dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1017f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dict_of_lists_from_dict_of_strings(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a456ca02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Correlation',\n",
       " 'FreqKurtosis',\n",
       " 'HRVRMSSD',\n",
       " 'Hurst',\n",
       " 'IQR',\n",
       " 'LempelZivComplexity',\n",
       " 'LyapunovExponent',\n",
       " 'MAD',\n",
       " 'Max',\n",
       " 'MaxFreqInd',\n",
       " 'Mean',\n",
       " 'MeanFreq',\n",
       " 'Min',\n",
       " 'SD',\n",
       " 'SampleEntropy',\n",
       " 'arCoeff'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDEAL_FEATURE_GROUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50d339cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hurst',\n",
       " 'LempelZivComplexity',\n",
       " 'LyapunovExponent',\n",
       " 'SampleEntropy',\n",
       " 'arCoeff'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_set  = INFO_FEATURE_GROUP \n",
    "#feature_set &= IDEAL_FEATURE_GROUP\n",
    "feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69e155b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_df = pd.read_csv('/tmp/existing_df.csv')\n",
    "new_df = pd.read_csv('/tmp/new_df.csv')\n",
    "label_cols = ['participant', 'condition', 'window index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d96a48f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predicament.data.features import add_features_to_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd73e172",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot merge as rows do not coincide",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43madd_features_to_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexisting_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_cols\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/external/predicament/predicament/data/features.py:424\u001b[0m, in \u001b[0;36madd_features_to_dataframe\u001b[0;34m(existing_df, new_df, label_cols)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlen(existing_df.index) = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(existing_df\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlen(new_df.index) = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(new_df\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 424\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerge_on = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmerge_on\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m merge_on:\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting_df[col].dtype = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexisting_df[col]\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)    \n",
      "\u001b[0;31mValueError\u001b[0m: Cannot merge as rows do not coincide"
     ]
    }
   ],
   "source": [
    "add_features_to_dataframe(\n",
    "        existing_df, new_df, label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87a896d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(existing_df.index) = 29698\n",
      "len(new_df.index) = 29698\n",
      "merge_on = ['participant', 'condition', 'window index']\n",
      "existing_df[col].dtype = object\n",
      "new_df[col].dtype = object\n",
      "existing_df[col].unique() = ['VG_01' 'VG_03' 'VG_05' 'VG_06' 'VG_07' 'VG_08' 'VG_09' 'VG_10' 'VH_01'\n",
      " 'VH_02' 'VH_03']\n",
      "new_df[col].unique() = ['VG_01' 'VG_03' 'VG_05' 'VG_06' 'VG_07' 'VG_08' 'VG_09' 'VG_10' 'VH_01'\n",
      " 'VH_02' 'VH_03']\n",
      "existing_df[col].dtype = int64\n",
      "new_df[col].dtype = int64\n",
      "existing_df[col].unique() = [0 1]\n",
      "new_df[col].unique() = [0 1]\n",
      "existing_df[col].dtype = int64\n",
      "new_df[col].dtype = int64\n",
      "existing_df[col].unique() = [   0    1    2 ... 1070 1071 1072]\n",
      "new_df[col].unique() = [   0    1    2 ... 1070 1071 1072]\n",
      "len(result_df.index) = 99104\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot merge as rows do not coincide",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m existing_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/tmp/existing_df.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m new_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/tmp/new_df.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot merge as rows do not coincide\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot merge as rows do not coincide"
     ]
    }
   ],
   "source": [
    "merge_on = label_cols\n",
    "left_cols = existing_df.columns\n",
    "right_cols = new_df.columns\n",
    "# new dataframe will overwrite any columns that are not labels and have\n",
    "# matching column names\n",
    "preserved_left_cols = [\n",
    "    c for c in left_cols if (c in merge_on) or (not c in right_cols) ]\n",
    "# hide the columns that will be overridden\n",
    "existing_df = existing_df.loc[:,preserved_left_cols]\n",
    "print(f\"len(existing_df.index) = {len(existing_df.index)}\")\n",
    "print(f\"len(new_df.index) = {len(new_df.index)}\")\n",
    "print(f\"merge_on = {merge_on}\")\n",
    "for col in merge_on:\n",
    "    print(f\"existing_df[col].dtype = {existing_df[col].dtype}\")    \n",
    "    print(f\"new_df[col].dtype = {new_df[col].dtype}\")    \n",
    "    print(f\"existing_df[col].unique() = {existing_df[col].unique()}\")    \n",
    "    print(f\"new_df[col].unique() = {new_df[col].unique()}\")    \n",
    "result_df = pd.merge(existing_df, new_df, how='inner', on=merge_on)\n",
    "print(f\"len(result_df.index) = {len(result_df.index)}\")\n",
    "if len(existing_df.index) != len(result_df.index):\n",
    "    existing_df.to_csv('/tmp/existing_df.csv')\n",
    "    new_df.to_csv('/tmp/new_df.csv')\n",
    "    raise ValueError('Cannot merge as rows do not coincide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bf5999",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
