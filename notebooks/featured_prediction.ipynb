{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44acde52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import re\n",
    "res_digit = r'[0-9]'\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn import preprocessing \n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from skopt import BayesSearchCV\n",
    "# parameter ranges are specified by one of below\n",
    "from skopt.space import Real, Categorical, Integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "832db706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thisdir = /home/luke/git/external/predicament/notebooks\n",
      "Adding parent directory to python path\n",
      "sys.path =\n",
      "['/home/luke/git/external/predicament/notebooks', '/home/luke/git/external/predicament', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/home/luke/.local/lib/python3.10/site-packages', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/lib/python3.10/dist-packages']\n"
     ]
    }
   ],
   "source": [
    "# This is a hack to make the library in the parent folder available for imoprts\n",
    "# A better solution is by np8 here:\n",
    "# https://stackoverflow.com/questions/714063/importing-modules-from-parent-folder\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "thisdir = sys.path[0]\n",
    "print(f\"thisdir = {thisdir}\")\n",
    "parentdir = os.path.dirname(thisdir)\n",
    "#print(f\"parentdir = {parentdir}\")\n",
    "if not parentdir in sys.path:\n",
    "    print(\"Adding parent directory to python path\")\n",
    "    sys.path.insert(1, parentdir)\n",
    "else:\n",
    "    print(\"Skipping adding parent direct to path (there already)\")\n",
    "\n",
    "print(f\"sys.path =\\n{sys.path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28f17581",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ensure relative path to data directory is sound\n",
    "# for the notebook we need to modify the BASE_DATA_FOLDER\n",
    "import os \n",
    "os.environ['PREDICAMENT_DATA_DIR'] =  '../data'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd724667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predicament.utils.file_utils import load_dataframe_and_config\n",
    "import predicament.utils.config_parser\n",
    "reload(predicament.utils.config_parser)\n",
    "from predicament.utils.config_parser import config_to_dict\n",
    "\n",
    "from predicament.utils.config import FEATURED_BASE_PATH\n",
    "from predicament.data.features import IDEAL_FEATURE_GROUP\n",
    "\n",
    "from predicament.evaluation.balancing import get_group_label_counts\n",
    "from predicament.evaluation.balancing import balance_data\n",
    "from predicament.evaluation.grouping import get_group_assignments\n",
    "from predicament.evaluation.staging import get_design_matrix\n",
    "from predicament.evaluation.results import output_model_best_from_results\n",
    "from predicament.evaluation.results import save_results_df_to_file\n",
    "\n",
    "from predicament.evaluation.hyperparameters import get_param_scopes\n",
    "from predicament.evaluation.hyperparameters import get_param_search_object\n",
    "\n",
    "from predicament.models.mlp_wrappers import ThreeHiddenLayerClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd59ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high level choices\n",
    "subdir = 'binary_dreem_4secs' # dataset\n",
    "held_out = 'participant'\n",
    "is_balanced = True # balance data set\n",
    "use_only_ideal_features = True # restrict to preferred ideal features\n",
    "standardise_data = False\n",
    "max_iter_opt = 200\n",
    "n_iter = 50 # number of iterations for your search\n",
    "new_search = True # restarts the search\n",
    "random_state = 43\n",
    "use_callback = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015e1a17",
   "metadata": {},
   "source": [
    "## Load featured data and balance if required\n",
    "\n",
    "Before running this, you will need to generate featured data. See README file for details. For the variable `subdir` above replace this with the subdirectory name of the featured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45e1ae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_data_dir = os.path.join(FEATURED_BASE_PATH,subdir)\n",
    "\n",
    "featured_df, featured_config = load_dataframe_and_config(\n",
    "    featured_data_dir, 'featured.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf468030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_rate: 250, n_samples = 1024, time: 4.096s, n_channels: 4\n"
     ]
    }
   ],
   "source": [
    "n_channels = featured_config['LOAD']['n_channels']\n",
    "data_format = featured_config['LOAD']['data_format']\n",
    "channels = featured_config['LOAD']['channels']\n",
    "participant_list = featured_config['LOAD']['participant_list']\n",
    "sample_rate = featured_config['LOAD']['sample_rate']\n",
    "Fs = sample_rate\n",
    "window_size = featured_config['LOAD']['window_size']\n",
    "time = window_size/sample_rate\n",
    "print(f\"sample_rate: {sample_rate}, n_samples = {window_size}, time: {time}s, n_channels: {n_channels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f91ee90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before balancing: subject_condition_counts = [[2657.    0.]\n",
      " [2638.    0.]\n",
      " [2072.  110.]\n",
      " [2423.  805.]\n",
      " [2423.  344.]\n",
      " [1501.  110.]\n",
      " [2122.  461.]\n",
      " [2282.  688.]\n",
      " [2079.  110.]\n",
      " [2027.  285.]\n",
      " [2540.  344.]\n",
      " [2540.  805.]]\n",
      "after balancing: subject_condition_counts = [[339.   0.]\n",
      " [339.   0.]\n",
      " [339. 110.]\n",
      " [339. 805.]\n",
      " [339. 344.]\n",
      " [339. 110.]\n",
      " [339. 461.]\n",
      " [339. 688.]\n",
      " [339. 110.]\n",
      " [339. 285.]\n",
      " [339. 344.]\n",
      " [339. 805.]]\n"
     ]
    }
   ],
   "source": [
    "if is_balanced:\n",
    "    # balance featured data\n",
    "    subject_condition_counts = get_group_label_counts(featured_df, 'participant', 'condition')\n",
    "    print(f\"before balancing: subject_condition_counts = {subject_condition_counts}\")\n",
    "    featured_df = balance_data(featured_df, group_col='participant', label_col='condition')\n",
    "    subject_condition_counts = get_group_label_counts(featured_df, 'participant', 'condition')\n",
    "\n",
    "    print(f\"after balancing: subject_condition_counts = {subject_condition_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eca41c",
   "metadata": {},
   "source": [
    "## Define model and hyperparamer search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "408a74c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimator = SVC()\n",
      "param_scopes = {'C': Real(low=0.1, high=10000.0, prior='log-uniform', transform='identity'), 'kernel': Categorical(categories=('rbf', 'sigmoid'), prior=None), 'gamma': Real(low=1e-09, high=1, prior='log-uniform', transform='identity'), 'coef0': Real(low=-1, high=1, prior='uniform', transform='identity'), 'shrinking': Categorical(categories=(True, False), prior=None)}\n"
     ]
    }
   ],
   "source": [
    "overrides = dict()\n",
    "excludes = list()\n",
    "# the base model to tune\n",
    "estimator = SVC()\n",
    "#estimator = GradientBoostingClassifier()\n",
    "#estimator = RandomForestClassifier()\n",
    "# max_iter_opt = 1\n",
    "#estimator = MLPClassifier(max_iter=max_iter_opt)\n",
    "# estimator = ThreeHiddenLayerClassifier()\n",
    "#excludes = ['layer3'] # for 2 (hidden) layer MLP (leave empty for 3 layer MLP)\n",
    "# excludes = ['layer2', 'layer3'] # for 1 (hidden) layer MLP\n",
    "print(f\"estimator = {estimator}\")\n",
    "\n",
    "# search_type = 'random_search'\n",
    "search_type = 'bayesian_optimization'\n",
    "\n",
    "# now create the parameter search object and run the hyperparameter search\n",
    "param_scopes = get_param_scopes(\n",
    "    search_type, estimator, excludes=excludes, **overrides)\n",
    "print(f\"param_scopes = {param_scopes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83039b8d",
   "metadata": {},
   "source": [
    "## Define data properties and data-split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d9fb7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_set = ['Correlation', 'FreqKurtosis', 'SampleEntropy', 'SD', 'Mean', 'Hurst', 'IQR', 'Max', 'LyapunovExponent', 'MaxFreqInd', 'LempelZivComplexity', 'MeanFreq', 'arCoeff', 'MAD', 'Min']\n"
     ]
    }
   ],
   "source": [
    "feature_set = featured_config['FEATURED']['feature_set']\n",
    "if use_only_ideal_features:\n",
    "    feature_set = list(IDEAL_FEATURE_GROUP.intersection(feature_set))\n",
    "    \n",
    "print(f\"feature_set = {feature_set}\")\n",
    "\n",
    "# extract input data\n",
    "# use all features in file\n",
    "feature_types, feature_names, designmtx = get_design_matrix(\n",
    "    featured_df, feature_set)\n",
    "# extract labels\n",
    "labels = featured_df['condition'].values.astype(int)\n",
    "\n",
    "if standardise_data:\n",
    "    scaler = preprocessing.StandardScaler().fit(designmtx)\n",
    "    designmtx = scaler.transform(designmtx)\n",
    "    \n",
    "# prepare Hold one group out cross validation\n",
    "held_out, groups, group_assignments = get_group_assignments(featured_df)\n",
    "n_groups = len(groups)\n",
    "# cross validation splits    \n",
    "group_kfold = GroupKFold(n_splits=n_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad18edb",
   "metadata": {},
   "source": [
    "## Define and Execute hyperparameter search strategy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda169a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n",
      "Best score: 0.3598576477794529\n",
      "Best parameters: [40.33985072818989, -0.493355159370347, 1.0837982698716211e-07, 'rbf', False]\n",
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n",
      "Best score: 0.3598576477794529\n",
      "Best parameters: [40.33985072818989, -0.493355159370347, 1.0837982698716211e-07, 'rbf', False]\n",
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n",
      "Best score: 0.3598576477794529\n",
      "Best parameters: [40.33985072818989, -0.493355159370347, 1.0837982698716211e-07, 'rbf', False]\n",
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n",
      "Best score: 0.3598576477794529\n",
      "Best parameters: [40.33985072818989, -0.493355159370347, 1.0837982698716211e-07, 'rbf', False]\n",
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n",
      "Best score: 0.4279802955698744\n",
      "Best parameters: [2.4206178382891803, 0.08349600882982644, 3.415679480068009e-05, 'rbf', False]\n",
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n",
      "Best score: 0.4449164327341321\n",
      "Best parameters: [1.0322186643632776, -0.1749273424136779, 6.413350264532572e-06, 'rbf', False]\n",
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n",
      "Best score: 0.4449164327341321\n",
      "Best parameters: [1.0322186643632776, -0.1749273424136779, 6.413350264532572e-06, 'rbf', False]\n",
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n",
      "Best score: 0.4449164327341321\n",
      "Best parameters: [1.0322186643632776, -0.1749273424136779, 6.413350264532572e-06, 'rbf', False]\n",
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n",
      "Best score: 0.4449164327341321\n",
      "Best parameters: [1.0322186643632776, -0.1749273424136779, 6.413350264532572e-06, 'rbf', False]\n",
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n",
      "Best score: 0.4449164327341321\n",
      "Best parameters: [1.0322186643632776, -0.1749273424136779, 6.413350264532572e-06, 'rbf', False]\n",
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n",
      "Best score: 0.4449164327341321\n",
      "Best parameters: [1.0322186643632776, -0.1749273424136779, 6.413350264532572e-06, 'rbf', False]\n",
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n",
      "Best score: 0.4449164327341321\n",
      "Best parameters: [1.0322186643632776, -0.1749273424136779, 6.413350264532572e-06, 'rbf', False]\n",
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n",
      "Best score: 0.5116273347813028\n",
      "Best parameters: [10000.0, 1.0, 1.0232142687791563e-05, 'rbf', True]\n",
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n",
      "Best score: 0.5116273347813028\n",
      "Best parameters: [10000.0, 1.0, 1.0232142687791563e-05, 'rbf', True]\n",
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n",
      "Best score: 0.5116273347813028\n",
      "Best parameters: [10000.0, 1.0, 1.0232142687791563e-05, 'rbf', True]\n",
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    }
   ],
   "source": [
    "# fix to avoid error in BayesSearchCV.fit\n",
    "import numpy as np\n",
    "np.int = int\n",
    "\n",
    "def create_callback_and_storage(param_search):\n",
    "    intermediate_results = []\n",
    "    # Define a custom callback function to store intermediate results\n",
    "    def on_step(optim_result):\n",
    "        # Store the current state of the optimization process\n",
    "        intermediate_results.append((optim_result.func_vals, optim_result.x_iters))\n",
    "\n",
    "        # Print out the best score and best parameters found so far\n",
    "        best_score = -optim_result.fun\n",
    "        print(\"Best score: %s\" % best_score)\n",
    "        print(\"Best parameters: %s\" % optim_result.x)\n",
    "    return on_step, intermediate_results\n",
    "\n",
    "\n",
    "param_search  = get_param_search_object(\n",
    "    search_type, estimator, param_scopes=param_scopes, \n",
    "    n_iter = n_iter, cv=group_kfold,\n",
    "    verbose=2, random_state=random_state, n_jobs=-1,\n",
    "    fit_params={'X': designmtx, 'y': labels, 'callbacks': None},\n",
    "    refit=False  # Ensure that the search does not refit the model with the best parameters found so far\n",
    ")\n",
    "\n",
    "if use_callback:\n",
    "    if new_search:\n",
    "        on_step, intermediate_results = create_callback_and_storage(param_search)\n",
    "    else:\n",
    "        param_search.fit_params['search_results'] = intermediate_results\n",
    "    try:\n",
    "        # Fit the random search model\n",
    "        _ = param_search.fit(X=designmtx, y=labels, groups=group_assignments, callback=on_step)\n",
    "    except Exception as e:\n",
    "        print(\"Exception occurred:\", str(e))\n",
    "    finally:\n",
    "        # Print or process intermediate results even if an error occurs\n",
    "        print(\"Intermediate Results:\")\n",
    "        for i, (scores, params) in enumerate(intermediate_results):\n",
    "            print(f\"Iteration {i + 1}: Scores - {scores}, Params - {params}\")\n",
    "        new_search = False\n",
    "        random_state = np.random.randint(100000)\n",
    "else:\n",
    "    _ = param_search.fit(X=designmtx, y=labels, groups=group_assignments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9466e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_search.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790e5687",
   "metadata": {},
   "source": [
    "## Saving and outputing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceedbcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(param_search.cv_results_)\n",
    "i = 0\n",
    "result_df.insert(i, 'model', str(estimator))\n",
    "i +=1\n",
    "result_df.insert(i, 'data format', data_format)\n",
    "i +=1\n",
    "result_df.insert(i, 'held out', held_out)\n",
    "i +=1\n",
    "result_df.insert(i, 'balanced', is_balanced)\n",
    "i +=1\n",
    "result_df.insert(i, 'n_splits', param_search.get_params()['cv'].get_n_splits())\n",
    "i +=1\n",
    "result_df.insert(i, 'feature set', str(feature_types))\n",
    "i +=1\n",
    "result_df.insert(i, 'window size', window_size)\n",
    "display(result_df)\n",
    "results_fname = f'{search_type}_{str(estimator)}'\n",
    "print(f\"Saving to {results_fname}\")\n",
    "save_results_df_to_file(result_df, results_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e34752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = output_model_best_from_results(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293fa9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = featured_config['WINDOWED']['label_cols']\n",
    "feature_types = list(feature_set)\n",
    "feature_types.sort()\n",
    "print(f\"# Feature Set:\\n{feature_types}\")\n",
    "derived_feature_names = []\n",
    "derived_feature_types = set([])\n",
    "for f in featured_df.columns:\n",
    "    if f in label_cols:\n",
    "        continue\n",
    "    elif (f[-1] == ']'):\n",
    "        if (f[:-1].rstrip('0123456789')[-1] == '['):\n",
    "            f = f[:-1].rstrip('0123456789')[:-1]\n",
    "    else:\n",
    "        f = f.rstrip('0123456789')\n",
    "    for type_ in feature_set:\n",
    "        if f.startswith(type_):\n",
    "            derived_feature_types.add(f)\n",
    "            break\n",
    "derived_feature_types = list(derived_feature_types)\n",
    "derived_feature_types.sort()\n",
    "output = ';'.join(derived_feature_types)\n",
    "print(f\"Derived Feature Types:\\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435f3abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0,0.7,51)\n",
    "N = len(result_df)\n",
    "props = np.empty(thresholds.size)\n",
    "for t, thresh in enumerate(thresholds):\n",
    "    count = np.sum(result_df['mean_test_score'] > thresh)\n",
    "    props[t] = count/N\n",
    "plt.plot(thresholds, props)\n",
    "plt.xlabel(\"mean test score\")\n",
    "plt.ylabel(\"proportion greater than\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3566f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7bca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_thresh = 0.6\n",
    "display_cols = [col for col in result_df.columns if col.startswith('param') or (col =='mean_test_score') or (col == 'std_test_score')]\n",
    "result_df[result_df['mean_test_score'] >= display_thresh][display_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c60452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or just show all rows\n",
    "result_df[display_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffc9872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
