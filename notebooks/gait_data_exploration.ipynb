{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518e95ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "import scipy.stats as spstats\n",
    "# fourier transform\n",
    "from scipy.fft import fft, ifft\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# from statsmodels.tsa.api import acf, graphics, pacf\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "# from statsmodels.tsa.ar_model import ar_select_order\n",
    "\n",
    "import os\n",
    "print(os.listdir(\".\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea1b6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a hack to make the library in the parent folder available for imoprts\n",
    "# A better solution is by np8 here:\n",
    "# https://stackoverflow.com/questions/714063/importing-modules-from-parent-folder\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "thisdir = sys.path[0]\n",
    "print(f\"thisdir = {thisdir}\")\n",
    "parentdir = os.path.dirname(thisdir)\n",
    "#print(f\"parentdir = {parentdir}\")\n",
    "if not parentdir in sys.path:\n",
    "    print(\"Adding parent directory to python path\")\n",
    "    sys.path.insert(1, parentdir)\n",
    "else:\n",
    "    print(\"Skipping adding parent direct to path (there already)\")\n",
    "\n",
    "print(f\"sys.path =\\n{sys.path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522e446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/other_datasets/uci_multivariate_gait_data/gait.csv\")\n",
    "print(f\"df.shape = {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63593078",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = df['subject'].unique()\n",
    "conditions = df['condition'].unique()\n",
    "legs = df['leg'].unique()\n",
    "joints = df['joint'].unique()\n",
    "replications = df['replication'].unique()\n",
    "times = df['time'].unique()\n",
    "print(f\"subjects = {subjects}\")\n",
    "print(f\"conditions = {conditions}\")\n",
    "print(f\"replications = {replications}\")\n",
    "print(f\"joints = {joints}\")\n",
    "print(f\"legs = {legs}\")\n",
    "print(f\"joints = {joints}\")\n",
    "print(f\"times = {times}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682765e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    subject_df = df[df['subject'] == subject]\n",
    "    print(f\"subject {subject}: has {subject_df.shape[0]} datapoints with {subject_df['time'].max()} time-points\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2dabeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_c1_r1_df = df[(df['subject'] == 1) &(df['condition'] == 1) &(df['replication'] == 1)]\n",
    "T = times.size\n",
    "n_channels = len(legs)*len(joints)\n",
    "\n",
    "s1_c1_r1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a06ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X111 = np.zeros((n_channels, T))\n",
    "for c,(leg, joint) in enumerate(itertools.product(legs,joints)):\n",
    "    X111[c,:] = s1_c1_r1_df[(s1_c1_r1_df['leg']==leg)&(s1_c1_r1_df['joint']==joint)]['angle'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2a7fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data looks like it has already been smoothed/filtered so\n",
    "# we will not be applying any filtering to this dataset\n",
    "fig, axs = plt.subplots(n_channels,1)\n",
    "for c,(leg, joint) in enumerate(itertools.product(legs,joints)):\n",
    "    axs[c].plot(times, X111[c,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6ffee5",
   "metadata": {},
   "source": [
    "## Aggregate functions\n",
    "\n",
    "Taken from:\n",
    "Public Domain Dataset for Human Activity Recognition Using Smartphones, (Anguita et al., 2013)\n",
    "\n",
    "| Function\t|\tDescription |\n",
    "|  :---:\t|\t:---:  |\n",
    "| mean\t|\tMean value |\n",
    "| std\t|\tStandard deviation |\n",
    "| mad\t|\tMedian absolute value |\n",
    "| max\t|\tLargest values in array |\n",
    "| min\t|\tSmallest value in array |\n",
    "| sma\t|\tSignal magnitude area |\n",
    "| energy\t|\tAverage sum of the squares |\n",
    "| iqr\t|\tInterquartile range |\n",
    "| entropy\t|\tSignal Entropy |\n",
    "| arCoeff\t|\tAutorregresion coefficients |\n",
    "| correlation\t|\tCorrelation coefficient |\n",
    "| maxFreqInd\t|\tLargest frequency component |\n",
    "| meanFreq\t|\tFrequency signal weighted average |\n",
    "| skewness\t|\tFrequency signal Skewness |\n",
    "| kurtosis\t|\tFrequency signal Kurtosis |\n",
    "| energyBand\t|\tEnergy of a frequency interval |\n",
    "| angle\t|\tAngle between two vectors |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c06ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sample entropy from Wikipedia\n",
    "from itertools import combinations\n",
    "from math import log\n",
    "\n",
    "def construct_templates(timeseries_data:list, m:int=2):\n",
    "    num_windows = len(timeseries_data) - m + 1\n",
    "    return [timeseries_data[x:x+m] for x in range(0, num_windows)]\n",
    "\n",
    "def get_matches(templates:list, r:float):\n",
    "    return len(list(filter(lambda x: is_match(x[0], x[1], r), combinations(templates, 2))))\n",
    "\n",
    "def is_match(template_1:list, template_2:list, r:float):\n",
    "    return all([abs(x - y) < r for (x, y) in zip(template_1, template_2)])\n",
    "\n",
    "def sample_entropy(timeseries_data:list, window_size:int, r:float):\n",
    "    B = get_matches(construct_templates(timeseries_data, window_size), r)\n",
    "    A = get_matches(construct_templates(timeseries_data, window_size+1), r)\n",
    "    return -log(A/B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0301892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlations(X):\n",
    "    return np.array([ e for i,r in enumerate(np.corrcoef(X)) for e in r[i+1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2f04e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# | mean\t|\tMean value |\n",
    "features111 = np.mean(X111,axis=1)\n",
    "# | std\t|\tStandard deviation |\n",
    "features111 = np.concatenate([features111, np.std(X111,axis=1)])\n",
    "# | mad\t|\tMedian absolute value/deviation |\n",
    "features111 = np.concatenate([features111, spstats.median_abs_deviation(X111,axis=1)])\n",
    "# | max\t|\tLargest values in array |\n",
    "features111 = np.concatenate([features111, np.max(X111,axis=1)])\n",
    "# | min\t|\tSmallest value in array |\n",
    "features111 = np.concatenate([features111, np.min(X111,axis=1)])\n",
    "print(f\"After min = {features111.shape}\")\n",
    "# | sma\t|\tSignal magnitude area |\n",
    "# not sure how useful for general signals\n",
    "# see: https://en.wikipedia.org/wiki/Signal_magnitude_area\n",
    "# seems similar to median absolute deviation\n",
    "# | energy\t|\tAverage sum of the squares |\n",
    "features111 = np.concatenate([features111, np.mean(X111**2,axis=1)])\n",
    "# | iqr\t|\tInterquartile range |\n",
    "features111 = np.concatenate([features111, spstats.iqr(X111,axis=1)])\n",
    "print(f\"after IQR = {features111.shape}\")\n",
    "# | entropy\t|\tSignal Entropy |\n",
    "# looks like sample entropy\n",
    "# see: https://academic.oup.com/biomethods/article/4/1/bpz016/5634143\n",
    "# see: https://www.mdpi.com/1099-4300/20/10/764\n",
    "window_size = 10\n",
    "tol = 0.05*(np.mean(np.std(X111,axis=1)))\n",
    "entropies = [\n",
    "    sample_entropy(list(ts), window_size, tol) for ts in X111]\n",
    "features111 = np.concatenate([features111, entropies])\n",
    "print(f\"after entropies = {features111.shape}\")\n",
    "# | arCoeff\t|\tAutorregresion coefficients |\n",
    "# using code from here:\n",
    "# https://www.statsmodels.org/dev/examples/notebooks/generated/autoregressions.html\n",
    "order = 3\n",
    "arCoeffs = np.empty((order+1, n_channels))\n",
    "for c in range(n_channels):\n",
    "    mod = AutoReg(X111[c,:], order, old_names=False)\n",
    "    res = mod.fit()\n",
    "    arCoeffs[:,c] = res.params\n",
    "features111 = np.concatenate([features111, arCoeffs.flatten()])\n",
    "print(f\"After arCoeffs = {features111.shape}\")\n",
    "# | correlation\t|\tCorrelation coefficient |\n",
    "features111 = np.concatenate([features111, correlations(X111)])\n",
    "print(f\"After correlations = {features111.shape}\")\n",
    "\n",
    "X111freq = fft(X111)\n",
    "print(f\"X111.shape = {X111.shape}\")\n",
    "print(f\"X111freq.shape = {X111freq.shape}\")\n",
    "# | maxFreqInd\t|\tLargest frequency component |\n",
    "print(f\"np.argmax(X111freq) = {np.argmax(X111freq, axis=1)}\")\n",
    "# | meanFreq\t|\tFrequency signal weighted average |\n",
    "# here frequency bands are given in multiples of the base frequency in dB\n",
    "freqBands = np.arange(X111freq.shape[1]).reshape((1,-1))\n",
    "print(f\"freqBands = {freqBands}\")\n",
    "# using intensity as the magnitude which is the square of the absolute value of the signal\n",
    "intensities = np.abs(X111freq)**2\n",
    "X111meanFreq = np.real(np.sum(freqBands*intensities,axis=1))/np.real(np.sum(intensities, axis=1))\n",
    "print(f\"X111meanFreq = {X111meanFreq}\")\n",
    "# an alternative mean frequency is to use p=1 (manhattan norm)\n",
    "X111meanFreqAlt = np.sum(freqBands*np.abs(X111freq),axis=1)/np.sum(np.abs(X111freq), axis=1)\n",
    "print(f\"X111meanFreqAlt = {X111meanFreqAlt}\")\n",
    "\n",
    "for i in range(X111freq.shape[0]):\n",
    "    plt.plot(freqBands.flatten(),np.abs(X111freq[i]))\n",
    "\n",
    "# | skewness\t|\tFrequency signal Skewness |\n",
    "# | kurtosis\t|\tFrequency signal Kurtosis |\n",
    "# | energyBand\t|\tEnergy of a frequency interval |\n",
    "# | angle\t|\tAngle between two vectors |\n",
    "\n",
    "features111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d87eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31499abd",
   "metadata": {},
   "source": [
    "## Things to explore\n",
    "\n",
    "[Hurst exponent](https://www.mdpi.com/1099-4300/23/12/1672), [Lyapunov exponent spectrum](), [Lempev-Ziv Complexity]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bdd651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predicament.data.features import convert_timeseries_to_features\n",
    "feature_set = set(\n",
    "    ['Mean', 'SD', 'MAD', 'Max', 'Min',# 'SMA',\n",
    "     'Energy', 'IQR', 'Entropy',\n",
    "    'arCoeff', 'Correlation', 'MaxFreqInd', 'MeanFreq', 'FreqSkewness',\n",
    "    'FreqKurtosis', 'EnergyBands'])\n",
    "features111alt = convert_timeseries_to_features(\n",
    "        X111, feature_set,\n",
    "        entropy_tol=0.05*(np.mean(np.std(X111,axis=1))))\n",
    "print(f\"features111.shape = {features111.shape}\")\n",
    "print(f\"features111alt.shape = {features111alt.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1776f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "tol = 0.05*(np.mean(np.std(X111,axis=1)))\n",
    "entropies = [\n",
    "    sample_entropy(list(ts), window_size, tol) for ts in X111]\n",
    "tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c868ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_timeseries_to_features(\n",
    "        X111, feature_set=set(['Entropy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352cb4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.std(X111,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f5e33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(~np.isclose(features111, features111alt[:87]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a6a550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
