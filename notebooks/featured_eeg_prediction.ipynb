{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44acde52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import re\n",
    "res_digit = r'[0-9]'\n",
    "\n",
    "# fourier transform\n",
    "from scipy.fft import fft, ifft\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "832db706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thisdir = /home/luke/git/external/predicament/notebooks\n",
      "Adding parent directory to python path\n",
      "sys.path =\n",
      "['/home/luke/git/external/predicament/notebooks', '/home/luke/git/external/predicament', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/home/luke/.local/lib/python3.10/site-packages', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/lib/python3.10/dist-packages']\n"
     ]
    }
   ],
   "source": [
    "# This is a hack to make the library in the parent folder available for imoprts\n",
    "# A better solution is by np8 here:\n",
    "# https://stackoverflow.com/questions/714063/importing-modules-from-parent-folder\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "thisdir = sys.path[0]\n",
    "print(f\"thisdir = {thisdir}\")\n",
    "parentdir = os.path.dirname(thisdir)\n",
    "#print(f\"parentdir = {parentdir}\")\n",
    "if not parentdir in sys.path:\n",
    "    print(\"Adding parent directory to python path\")\n",
    "    sys.path.insert(1, parentdir)\n",
    "else:\n",
    "    print(\"Skipping adding parent direct to path (there already)\")\n",
    "\n",
    "print(f\"sys.path =\\n{sys.path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28f17581",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ensure relative path to data directory is sound\n",
    "# for the notebook we need to modify the BASE_DATA_FOLDER\n",
    "import os \n",
    "os.environ['PREDICAMENT_DATA_DIR'] =  '../data'\n",
    "\n",
    "from predicament.utils.config import DREEM_EEG_CHANNELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd724667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predicament.data.timeseries import create_participant_data_edf_only\n",
    "from predicament.data.windowed import window_all_participants_data\n",
    "from predicament.data.windowed import merge_condition_data\n",
    "from predicament.data.partitioning import between_subject_cv_partition\n",
    "\n",
    "from predicament.data.features import MAXIMAL_FEATURE_GROUP\n",
    "from predicament.data.features import STATS_FEATURE_GROUP\n",
    "from predicament.data.features import INFO_FEATURE_GROUP\n",
    "from predicament.data.features import FREQ_FEATURE_GROUP\n",
    "from predicament.data.features import convert_timeseries_to_features\n",
    "from prepare_evaluation_data import load_dataframe_and_config\n",
    "\n",
    "from predicament.evaluation.balancing import get_group_label_counts\n",
    "from predicament.evaluation.balancing import balance_data\n",
    "# from predicament.data.datasets import propose_balanced_subject_condition_counts\n",
    "# from predicament.data.datasets import subsample_proposed_subject_condition_counts\n",
    "# from predicament.data.datasets import get_subject_condition_counts\n",
    "\n",
    "from predicament.evaluation.results import output_model_best_from_results\n",
    "from predicament.evaluation.results import save_results_df_to_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015e1a17",
   "metadata": {},
   "source": [
    "## Load features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45e1ae95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fs: 250, n_samples = 1024, time: 4.096s, n_channels: 5\n"
     ]
    }
   ],
   "source": [
    "featured_df, featured_config = load_dataframe_and_config(\n",
    "    '../data/featured/20231129210920/', 'featured.csv')\n",
    "n_channels = int(featured_config['LOAD']['n_channels'])\n",
    "channels = json.loads(featured_config['LOAD']['channels'].replace(\"'\",'\"'))\n",
    "participant_list = json.loads(featured_config['LOAD']['participant_list'].replace(\"'\",'\"'))\n",
    "Fs = int(featured_config['LOAD']['sample_rate'])\n",
    "window_size = int(featured_config['LOAD']['window_size'])\n",
    "time = window_size/Fs\n",
    "print(f\"Fs: {Fs}, n_samples = {window_size}, time: {time}s, n_channels: {n_channels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "408a74c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['part_ID', 'condition', 'start time', 'Mean0', 'Mean1', 'Mean2',\n",
       "       'Mean3', 'Mean4', 'SD0', 'SD1', 'SD2', 'SD3', 'SD4', 'MAD0', 'MAD1',\n",
       "       'MAD2', 'MAD3', 'MAD4', 'Max0', 'Max1', 'Max2', 'Max3', 'Max4', 'Min0',\n",
       "       'Min1', 'Min2', 'Min3', 'Min4', 'Energy0', 'Energy1', 'Energy2',\n",
       "       'Energy3', 'Energy4', 'IQR0', 'IQR1', 'IQR2', 'IQR3', 'IQR4',\n",
       "       'Correlation0', 'Correlation1', 'Correlation2', 'Correlation3',\n",
       "       'Correlation4', 'Correlation5', 'Correlation6', 'Correlation7',\n",
       "       'Correlation8', 'Correlation9', 'arCoeff0', 'arCoeff1', 'arCoeff2',\n",
       "       'arCoeff3', 'arCoeff4', 'arCoeff5', 'arCoeff6', 'arCoeff7', 'arCoeff8',\n",
       "       'arCoeff9', 'arCoeff10', 'arCoeff11', 'arCoeff12', 'arCoeff13',\n",
       "       'arCoeff14', 'arCoeff15', 'arCoeff16', 'arCoeff17', 'arCoeff18',\n",
       "       'arCoeff19', 'LyapunovExponent0', 'LyapunovExponent1',\n",
       "       'LyapunovExponent2', 'LyapunovExponent3', 'LyapunovExponent4',\n",
       "       'MaxFreqInd0', 'MaxFreqInd1', 'MaxFreqInd2', 'MaxFreqInd3',\n",
       "       'MaxFreqInd4', 'MeanFreq0', 'MeanFreq1', 'MeanFreq2', 'MeanFreq3',\n",
       "       'MeanFreq4', 'FreqSkewness0', 'FreqSkewness1', 'FreqSkewness2',\n",
       "       'FreqSkewness3', 'FreqSkewness4', 'FreqKurtosis0', 'FreqKurtosis1',\n",
       "       'FreqKurtosis2', 'FreqKurtosis3', 'FreqKurtosis4', 'SampleEntropy0',\n",
       "       'SampleEntropy1', 'SampleEntropy2', 'SampleEntropy3', 'SampleEntropy4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featured_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2611d2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns_to_use = ['Mean0', 'Mean1', 'Mean2', 'Mean3', 'Mean4', 'SD0', 'SD1', 'SD2', 'SD3', 'SD4', 'MAD0', 'MAD1', 'MAD2', 'MAD3', 'MAD4', 'Max0', 'Max1', 'Max2', 'Max3', 'Max4', 'Min0', 'Min1', 'Min2', 'Min3', 'Min4', 'Energy0', 'Energy1', 'Energy2', 'Energy3', 'Energy4', 'IQR0', 'IQR1', 'IQR2', 'IQR3', 'IQR4', 'Correlation0', 'Correlation1', 'Correlation2', 'Correlation3', 'Correlation4', 'Correlation5', 'Correlation6', 'Correlation7', 'Correlation8', 'Correlation9', 'arCoeff0', 'arCoeff1', 'arCoeff2', 'arCoeff3', 'arCoeff4', 'arCoeff5', 'arCoeff6', 'arCoeff7', 'arCoeff8', 'arCoeff9', 'arCoeff10', 'arCoeff11', 'arCoeff12', 'arCoeff13', 'arCoeff14', 'arCoeff15', 'arCoeff16', 'arCoeff17', 'arCoeff18', 'arCoeff19', 'MaxFreqInd0', 'MaxFreqInd1', 'MaxFreqInd2', 'MaxFreqInd3', 'MaxFreqInd4', 'MeanFreq0', 'MeanFreq1', 'MeanFreq2', 'MeanFreq3', 'MeanFreq4', 'FreqSkewness0', 'FreqSkewness1', 'FreqSkewness2', 'FreqSkewness3', 'FreqSkewness4', 'FreqKurtosis0', 'FreqKurtosis1', 'FreqKurtosis2', 'FreqKurtosis3', 'FreqKurtosis4']\n"
     ]
    }
   ],
   "source": [
    "features_to_use = set(\n",
    "    ['Mean', 'SD', 'MAD', 'Max', 'Min',# 'SMA',\n",
    "      'Energy', 'IQR', # 'Entropy',\n",
    "     'arCoeff', 'Correlation', 'MaxFreqInd', 'MeanFreq', 'FreqSkewness',\n",
    "    'FreqKurtosis' # , 'EnergyBands'\n",
    "    ])\n",
    "columns_to_use = [ col for col in featured_df.columns if re.sub(res_digit, '', col) in features_to_use ]\n",
    "print(f\"columns_to_use = {columns_to_use}\")\n",
    "\n",
    "# designmtx = featured_df[columns_to_use].values \n",
    "# condition_data = featured_df['condition'].values.astype(int)\n",
    "# subject_data_names = featured_df['part_ID']\n",
    "\n",
    "# design2d = TSNE(n_components=2, init='random', perplexity=3).fit_transform(designmtx)\n",
    "# print(f\"design2d.shape = {design2d.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be838edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proposed_subject_condition_counts = propose_balanced_subject_condition_counts(\n",
    "#     featured_df)\n",
    "# print(f\"proposed_subject_condition_counts = {proposed_subject_condition_counts}\")\n",
    "# proposed_condition_counts = np.sum(proposed_subject_condition_counts, axis=0)\n",
    "# print(f\"proposed_condition_counts = {proposed_condition_counts}\")\n",
    "# proposed_class_balance = proposed_condition_counts/np.sum(proposed_condition_counts)\n",
    "# print(f\"proposed_class_balance = {proposed_class_balance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37721cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def subsample_proposed_subject_condition_counts(df, proposed_sc_counts):\n",
    "#     subjects = np.unique(df['part_ID'])\n",
    "#     n_subjects = subjects.size\n",
    "#     conditions = np.unique(df['condition'])\n",
    "#     n_conditions = conditions.size\n",
    "#     new_df = pd.DataFrame(columns=df.columns)\n",
    "#     for s, subject in enumerate(subjects):\n",
    "#         for c, condition in enumerate(conditions):\n",
    "#             sc_df = df[(df['part_ID'] == subject) & (df['condition'] == condition)]\n",
    "#             new_df = pd.concat(\n",
    "#                 (new_df, sc_df.sample(proposed_sc_counts[s,c], replace=False)))\n",
    "#     return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50a5d372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_subject_condition_counts(df):\n",
    "#     subjects = np.unique(df['part_ID'])\n",
    "#     n_subjects = subjects.size\n",
    "#     conditions = np.unique(df['condition'])\n",
    "#     n_conditions = conditions.size\n",
    "#     subject_condition_counts = np.zeros((subjects.size,conditions.size))\n",
    "\n",
    "#     for s, part_ID in enumerate(subjects): \n",
    "#         subj_df = df[df['part_ID'] == part_ID]\n",
    "#         for c, cond in enumerate(conditions):\n",
    "#             count = len(subj_df[subj_df['condition'] == cond])\n",
    "#     #         print(f\"count = {count}\")\n",
    "#     #         print(f\"{s}-{c} : {count}/{tot} = {count/tot}\")\n",
    "#             subject_condition_counts[s,c] = count\n",
    "#     return subject_condition_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d0cc036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# featured_df = subsample_proposed_subject_condition_counts(\n",
    "#     featured_df, proposed_subject_condition_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e283660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before balancing: subject_condition_counts = [[ 813.  461.  461.  461.  461.]\n",
      " [ 813.  344.  461.  559.  461.]\n",
      " [ 813.  110.  344.  461.  344.]\n",
      " [ 813.  227.  461.  461.  461.]\n",
      " [ 813.  578.  344.  227.  461.]\n",
      " [   0.  110.    0. 1047.  344.]\n",
      " [ 832.  422.    0.  504.  364.]\n",
      " [ 813.  578.  461.  344.   86.]\n",
      " [ 813.  578.    0.  344.  344.]\n",
      " [ 803.  325.    0.  551.  348.]\n",
      " [ 930.  461.  344.  461.  344.]\n",
      " [ 696.  578.  461.  344.  461.]]\n",
      "after balancing: subject_condition_counts = [[304. 322. 461. 283. 296.]\n",
      " [304. 322. 461. 283. 296.]\n",
      " [304. 110. 344. 283. 296.]\n",
      " [304. 227. 461. 283. 296.]\n",
      " [304. 322. 344. 227. 296.]\n",
      " [  0. 110.   0. 283. 296.]\n",
      " [304. 322.   0. 283. 296.]\n",
      " [304. 322. 461. 283.  86.]\n",
      " [304. 322.   0. 283. 296.]\n",
      " [304. 322.   0. 283. 296.]\n",
      " [304. 322. 344. 283. 296.]\n",
      " [304. 322. 461. 283. 296.]]\n"
     ]
    }
   ],
   "source": [
    "# balance featured data\n",
    "subject_condition_counts = get_group_label_counts(featured_df, 'part_ID', 'condition')\n",
    "print(f\"before balancing: subject_condition_counts = {subject_condition_counts}\")\n",
    "featured_df = balance_data(featured_df, group_col='part_ID', label_col='condition')\n",
    "subject_condition_counts = get_group_label_counts(featured_df, 'part_ID', 'condition')\n",
    "\n",
    "print(f\"after balancing: subject_condition_counts = {subject_condition_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b720f6e",
   "metadata": {},
   "source": [
    "## Visualising data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d167e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = np.unique(subject_data_names)\n",
    "subject_data = np.empty(subject_data_names.shape, dtype=int)\n",
    "for s, sub in enumerate(subjects):\n",
    "    subject_data[subject_data_names==sub] = s\n",
    "print(f\"designmtx.shape = {designmtx.shape}\")\n",
    "print(f\"condition_data.shape = {condition_data.shape}\")\n",
    "print(f\"subject_data.shape = {subject_data.shape}\")\n",
    "    \n",
    "conditions = np.unique(condition_data)\n",
    "markers = ['v', '^', '<', '>', 's', '*', '+' , 'x', 'D', '.']\n",
    "colours = ['b','g','r','y','k']\n",
    "cmap = plt.cm.rainbow\n",
    "norm = colors.BoundaryNorm(np.arange(np.min(subject_data)-0.5,np.max(subject_data)+0.5), cmap.N)\n",
    "\n",
    "plt.scatter(\n",
    "    design2d[:,0], design2d[:,1], c=subject_data, norm=norm, s=0.5, edgecolor='none')\n",
    "plt.colorbar(\n",
    "    ticks=np.arange(subjects.size))\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "cmap = plt.cm.rainbow\n",
    "norm = colors.BoundaryNorm(np.arange(np.min(condition_data)-0.5,np.max(condition_data)+0.5), cmap.N)\n",
    "\n",
    "plt.scatter(\n",
    "    design2d[:,0], design2d[:,1], c=condition_data, norm=norm, s=0.5, edgecolor='none')\n",
    "plt.colorbar(\n",
    "    ticks=np.arange(conditions.size))\n",
    "# for s,subject in enumerate(subjects):\n",
    "#     marker = s\n",
    "#     for c, condition in enumerate(conditions):\n",
    "#         _filter = (condition_data==condition) &(subject_data==subject)\n",
    "#         #print(f\"condition = {condition}\")\n",
    "#         #print(f\"np.sum(_filter) = {np.sum(_filter)}\")\n",
    "#         plt.plot(design2d[_filter,0], design2d[_filter,1], ls='None', marker=markers[s], color=colours[c], markeredgewidth=0.1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86e3223",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9fb7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        min_samples_leaf=5, random_state=0\n",
    "    ),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(\n",
    "        max_leaf_nodes=15, random_state=0\n",
    "    ),\n",
    "    \"MLP\":  MLPClassifier(max_iter=100)\n",
    "}\n",
    "param_grids = {\n",
    "    \"Random Forest\": {\"n_estimators\": [10, 20, 50, 100]},\n",
    "    \"Gradient Boosting\": {\"n_estimators\": [10, 20, 50, 100]},\n",
    "    \"MLP\": {\n",
    "        'hidden_layer_sizes': [(10,),(20,),(50,),(100,)],\n",
    "        'activation': ['relu'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'alpha': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05],\n",
    "        'learning_rate': ['constant','adaptive'],\n",
    "    }\n",
    "}\n",
    "\n",
    "# determine the columns to use\n",
    "features_to_use = set(\n",
    "    ['Mean', 'SD', 'MAD', 'Max', 'Min',# 'SMA',\n",
    "      'Energy', 'IQR', # 'Entropy',\n",
    "     'arCoeff', 'Correlation', 'MaxFreqInd', 'MeanFreq', 'FreqSkewness',\n",
    "    'FreqKurtosis' # , 'EnergyBands'\n",
    "    ])\n",
    "columns_to_use = [ col for col in featured_df.columns if re.sub(res_digit, '', col) in features_to_use ]\n",
    "print(f\"columns_to_use = {columns_to_use}\")\n",
    "\n",
    "designmtx = featured_df[columns_to_use].to_numpy()\n",
    "\n",
    "condition_data = featured_df['condition'].values.astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f34496d",
   "metadata": {},
   "source": [
    "## Hold one group out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b39cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard cross-validation\n",
    "# Match all digits in the string and replace them with an empty string\n",
    "# new_string = re.sub(pattern, '', string1)\n",
    "subjects = np.unique(featured_df['part_ID'])\n",
    "n_subjects = len(subjects)\n",
    "groups = np.empty(len(featured_df), dtype=int)\n",
    "for s, sub in enumerate(subjects):\n",
    "    groups[featured_df['part_ID']==sub] = s\n",
    "# cross validation splits    \n",
    "group_kfold = GroupKFold(n_splits=n_subjects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab798c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame()\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grids[name],\n",
    "        return_train_score=True,\n",
    "        cv=group_kfold,\n",
    "    ).fit(X=designmtx, y=condition_data, groups=groups)\n",
    "    result_df = pd.DataFrame(grid_search.cv_results_)\n",
    "    #test_df['model'] = result['model']\n",
    "    result_df.insert(0, 'model', name)\n",
    "    result_df.insert(1, 'held out', 'subject')\n",
    "    result_df.insert(2, 'feature set', str(features_to_use))\n",
    "    display(result_df)\n",
    "    results_df = pd.concat((results_df, result_df))\n",
    "    #result = {\"model\": name, \"cv_results\": pd.DataFrame(grid_search.cv_results_)}\n",
    "    #results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea485d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import predicament.evaluation.results\n",
    "reload(predicament.evaluation.results)\n",
    "from predicament.evaluation.results import test\n",
    "from predicament.evaluation.results import save_results_df_to_file\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941c427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results_df_to_file(results_df, 'balanced_eeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e34752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_best_from_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ef42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d15480",
   "metadata": {},
   "source": [
    "## Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15feda71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]# Create the random grid\n",
    "random_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_features': max_features,\n",
    "    'max_depth': max_depth,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db20d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator = rf,\n",
    "    param_distributions = random_grid,\n",
    "    n_iter = 100,\n",
    "    cv=group_kfold,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs = -1)\n",
    "\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X=designmtx, y=condition_data, groups=groups)\n",
    "random_result_df = pd.DataFrame(rf_random.cv_results_)\n",
    "save_results_df_to_file(random_result_df, 'random_search_random_forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05122f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # focus on MLP model\n",
    "# name = \"MLP\"  \n",
    "# mlp_param_grid = dict(\n",
    "#     hidden_layer_sizes=[(10*i,) for i in range(1,21)],\n",
    "# #     activation=['tanh', 'relu'],\n",
    "# #     solver=['sgd', 'adam'],\n",
    "#     alpha=np.logspace(-5,-1,13),\n",
    "# #     learning_rate=['constant','adaptive'],\n",
    "#     #learning_rate_init=np.logspace(-5,-1,13),\n",
    "#     max_iter=[50, 100, 150, 200]\n",
    "#     )\n",
    "\n",
    "# for param in mlp_param_grid.keys():\n",
    "#     these_static_params = { k:v for k, v in mlp_best_params.items() if k != param }\n",
    "#     model = MLPClassifier(**these_static_params)\n",
    "#     this_param_grid = { k:v for k,v in mlp_param_grid.items() if k == param }\n",
    "#     grid_search = GridSearchCV(\n",
    "#         estimator=model,\n",
    "#         param_grid=this_param_grid,\n",
    "#         return_train_score=True,\n",
    "#         cv=group_kfold,\n",
    "#     ).fit(X=designmtx, y=condition_data, groups=groups)\n",
    "#     result_df = pd.DataFrame(grid_search.cv_results_)\n",
    "#     #test_df['model'] = result['model']\n",
    "#     result_df.insert(0, 'model', name)\n",
    "#     result_df.insert(1, 'held out', 'subject')\n",
    "#     result_df.insert(2, 'feature set', str(features_to_use))\n",
    "#     display(result_df)\n",
    "#     results_df = pd.concat((results_df, result_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff5afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian optimisation\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "# parameter ranges are specified by one of below\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X, y = load_iris(True)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "#      train_size=0.75,\n",
    "#     random_state=0)\n",
    "\n",
    "# estimator = SVC()\n",
    "# search_spaces =      {\n",
    "#          'C': Real(1e-6, 1e+6, prior='log-uniform'),\n",
    "#          'gamma': Real(1e-6, 1e+1, prior='log-uniform'),\n",
    "#          'degree': Integer(1,8),\n",
    "#          'kernel': Categorical(['linear', 'poly', 'rbf']),\n",
    "#      }\n",
    "estimator = MLPClassifier(max_iter=100)\n",
    "#     \"MLP\": {\n",
    "#         'hidden_layer_sizes': [(10,),(20,),(50,),(100,)],\n",
    "#         'activation': ['relu'],\n",
    "#         'solver': ['sgd', 'adam'],\n",
    "#         'alpha': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05],\n",
    "#         'learning_rate': ['constant','adaptive'],\n",
    "#     }\n",
    "search_spaces =      {\n",
    "        'hidden_layer_sizes': Categorical([ (n,) for n in range(10,200,10)]),\n",
    "        'activation': Categorical(['tanh', 'relu']),\n",
    "        'solver': Categorical(['sgd', 'adam']),\n",
    "        'alpha': Real(1e-6, 1e+1, prior='log-uniform'),\n",
    "        'learning_rate': Categorical(['constant','adaptive']),\n",
    "     }\n",
    "\n",
    "# log-uniform: understand as search over p = exp(x) by varying x\n",
    "param_search = BayesSearchCV(\n",
    "    estimator, search_spaces,\n",
    "    cv=group_kfold, verbose=2, random_state=42, n_iter=50)\n",
    "\n",
    "# executes bayesian optimization\n",
    "_ = param_search.fit(X=designmtx, y=condition_data, groups=groups)\n",
    "result_df = pd.DataFrame(param_search.cv_results_)\n",
    "result_df.insert(0, 'model', name)\n",
    "result_df.insert(1, 'held out', 'subject')\n",
    "result_df.insert(2, 'feature set', str(features_to_use))\n",
    "display(result_df)\n",
    "# results_df = pd.concat((results_df, result_df))\n",
    "\n",
    "save_results_df_to_file(result_df, 'bayes_search')\n",
    "\n",
    "# # model can be saved, used for predictions or scoring\n",
    "# print(opt.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621bcd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(result_df[result_df['mean_test_score'] == result_df['mean_test_score'].max()].loc[37])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bf1262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # balancing data\n",
    "# ## first get the proposed new counts of each subject and condition\n",
    "# def propose_balanced_subject_condition_counts(df):\n",
    "#     subjects = np.unique(df['part_ID'])\n",
    "#     n_subjects = subjects.size\n",
    "#     conditions = np.unique(df['condition'])\n",
    "#     n_conditions = conditions.size\n",
    "# #     tot = len(df)\n",
    "#     subject_condition_counts = np.zeros((subjects.size,conditions.size), dtype=int)\n",
    "\n",
    "#     for s, part_ID in enumerate(subjects): \n",
    "#         subj_df = df[df['part_ID'] == part_ID]\n",
    "#         for c, cond in enumerate(conditions):\n",
    "#             count = len(subj_df[subj_df['condition'] == cond])\n",
    "#     #         print(f\"count = {count}\")\n",
    "#     #         print(f\"{s}-{c} : {count}/{tot} = {count/tot}\")\n",
    "#             subject_condition_counts[s,c] = count\n",
    "\n",
    "#     # print(f\"subject_condition_counts = {subject_condition_counts}\")\n",
    "#     condition_counts = np.sum(subject_condition_counts, axis=0)\n",
    "#     # print(f\"condition_counts = {condition_counts}\")\n",
    "#     desired_condition_count = np.min(condition_counts)\n",
    "#     # print(f\"desired_condition_count = {desired_condition_count}\")\n",
    "#     proposed_subject_condition_counts = np.ones(subject_condition_counts.shape)*desired_condition_count//len(subjects)\n",
    "#     proposed_subject_condition_counts = np.minimum(subject_condition_counts, proposed_subject_condition_counts)\n",
    "#     to_allocate = desired_condition_count - np.sum(proposed_subject_condition_counts, axis=0)\n",
    "#     while np.any(to_allocate >= n_subjects):\n",
    "#         proposed_subject_condition_counts += (to_allocate//n_subjects).reshape((1,-1))\n",
    "#         proposed_subject_condition_counts = np.minimum(subject_condition_counts,proposed_subject_condition_counts)\n",
    "#         to_allocate = desired_condition_count - np.sum(proposed_subject_condition_counts, axis=0)\n",
    "#     # print(f\"proposed_subject_condition_counts = {proposed_subject_condition_counts}\")\n",
    "#     # print(f\"to_allocate = {to_allocate}\")\n",
    "#     proposed_condition_counts = np.sum(proposed_subject_condition_counts, axis=0)\n",
    "#     # print(f\"proposed_condition_counts = {proposed_condition_counts}\")\n",
    "#     proposed_class_balance = proposed_condition_counts/np.sum(proposed_condition_counts)\n",
    "#     # print(f\"proposed_class_balance = {proposed_class_balance}\")\n",
    "#     # print(\"add one to cells with the capacity...\")\n",
    "#     while np.any(to_allocate > 0):\n",
    "#         proposed_subject_condition_counts[(subject_condition_counts > proposed_subject_condition_counts) & (to_allocate.reshape((1,-1)) > 0)] += 1\n",
    "#         #print(f\"proposed_subject_condition_counts = {proposed_subject_condition_counts}\")\n",
    "#         to_allocate = desired_condition_count - np.sum(proposed_subject_condition_counts, axis=0)\n",
    "#         #print(f\"to_allocate = {to_allocate}\")\n",
    "#     return proposed_subject_condition_counts.astype(int)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ac6d1c",
   "metadata": {},
   "source": [
    "# Balanced data \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27102ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = np.unique(balanced_featured_df['part_ID'])\n",
    "n_subjects = len(subjects)\n",
    "groups = np.empty(len(balanced_featured_df), dtype=int)\n",
    "for s, sub in enumerate(subjects):\n",
    "    groups[balanced_featured_df['part_ID']==sub] = s\n",
    "    \n",
    "features_to_use = set(\n",
    "    ['Mean', 'SD', 'MAD', 'Max', 'Min',# 'SMA',\n",
    "      'Energy', 'IQR', # 'Entropy',\n",
    "     'arCoeff', 'Correlation', 'MaxFreqInd', 'MeanFreq', 'FreqSkewness',\n",
    "    'FreqKurtosis' # , 'EnergyBands'\n",
    "    ])\n",
    "columns_to_use = [ col for col in balanced_featured_df.columns if re.sub(res_digit, '', col) in features_to_use ]\n",
    "print(f\"columns_to_use = {columns_to_use}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdedeb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "designmtx = balanced_featured_df[columns_to_use].values \n",
    "condition_data = balanced_featured_df['condition'].values.astype(int)\n",
    "\n",
    "balanced_results_df = pd.DataFrame()\n",
    "group_kfold = GroupKFold(n_splits=n_subjects)\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grids[name],\n",
    "        return_train_score=True,\n",
    "        cv=group_kfold,\n",
    "    ).fit(X=designmtx, y=condition_data, groups=groups)\n",
    "    result_df = pd.DataFrame(grid_search.cv_results_)\n",
    "    #test_df['model'] = result['model']\n",
    "    result_df.insert(0, 'model', name)\n",
    "    result_df.insert(1, 'held out', 'subject')\n",
    "    result_df.insert(2, 'feature set', str(features_to_use))\n",
    "    display(result_df)\n",
    "    balanced_results_df = pd.concat((balanced_results_df, result_df))\n",
    "    #result = {\"model\": name, \"cv_results\": pd.DataFrame(grid_search.cv_results_)}\n",
    "    #results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c7b05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def output_model_best_from_results(results_df):\n",
    "#     for model in np.unique(results_df['model']):\n",
    "#         model_max_test_score = results_df[results_df['model'] == model]['mean_test_score'].max()\n",
    "#         model_max_std_test_score = results_df[(results_df['model'] == model) & (results_df['mean_test_score']==model_max_test_score)]['std_test_score'].to_numpy()[0]\n",
    "#         print(f\"{model}: max_test_score= {model_max_test_score}, max_std_test_score= {model_max_std_test_score}\")\n",
    "#         d = results_df[results_df['mean_test_score'] == model_max_test_score]['params']\n",
    "#         for k,v in d.items():\n",
    "#             model_best_params = v\n",
    "#             print(f\"best params: {v}\")\n",
    "#         print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6893764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_results_df_to_file(df, shortname, timestamp=True):\n",
    "#     import datetime\n",
    "#     nowstr = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "#     if timestamp:\n",
    "#         name = shortname + '_' + nowstr\n",
    "#     else:\n",
    "#         name = shortname\n",
    "#     fpath = f'../data/results/{name}.csv'\n",
    "#     print(f\"saving to {fpath}\")\n",
    "#     results_df.to_csv(fpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6856f057",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
