{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44acde52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import re\n",
    "res_digit = r'[0-9]'\n",
    "\n",
    "# fourier transform\n",
    "from scipy.fft import fft, ifft\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832db706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a hack to make the library in the parent folder available for imoprts\n",
    "# A better solution is by np8 here:\n",
    "# https://stackoverflow.com/questions/714063/importing-modules-from-parent-folder\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "thisdir = sys.path[0]\n",
    "print(f\"thisdir = {thisdir}\")\n",
    "parentdir = os.path.dirname(thisdir)\n",
    "#print(f\"parentdir = {parentdir}\")\n",
    "if not parentdir in sys.path:\n",
    "    print(\"Adding parent directory to python path\")\n",
    "    sys.path.insert(1, parentdir)\n",
    "else:\n",
    "    print(\"Skipping adding parent direct to path (there already)\")\n",
    "\n",
    "print(f\"sys.path =\\n{sys.path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f17581",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ensure relative path to data directory is sound\n",
    "# for the notebook we need to modify the BASE_DATA_FOLDER\n",
    "import os \n",
    "os.environ['PREDICAMENT_DATA_DIR'] =  '../data'\n",
    "\n",
    "from predicament.utils.config import DREEM_EEG_CHANNELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd724667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predicament.data.timeseries import create_participant_data_edf_only\n",
    "from predicament.data.windowed import window_all_participants_data\n",
    "from predicament.data.windowed import merge_condition_data\n",
    "from predicament.data.partitioning import between_subject_cv_partition\n",
    "\n",
    "from predicament.data.features import MAXIMAL_FEATURE_GROUP\n",
    "from predicament.data.features import STATS_FEATURE_GROUP\n",
    "from predicament.data.features import INFO_FEATURE_GROUP\n",
    "from predicament.data.features import FREQ_FEATURE_GROUP\n",
    "from predicament.data.features import convert_timeseries_to_features\n",
    "from prepare_evaluation_data import load_dataframe_and_config\n",
    "\n",
    "from predicament.evaluation.balancing import get_group_label_counts\n",
    "from predicament.evaluation.balancing import balance_data\n",
    "# from predicament.data.datasets import propose_balanced_subject_condition_counts\n",
    "# from predicament.data.datasets import subsample_proposed_subject_condition_counts\n",
    "# from predicament.data.datasets import get_subject_condition_counts\n",
    "\n",
    "from predicament.evaluation.results import output_model_best_from_results\n",
    "from predicament.evaluation.results import save_results_df_to_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015e1a17",
   "metadata": {},
   "source": [
    "## Load features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e1ae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_df, featured_config = load_dataframe_and_config(\n",
    "    '../data/featured/20231129210920/', 'featured.csv')\n",
    "n_channels = int(featured_config['LOAD']['n_channels'])\n",
    "channels = json.loads(featured_config['LOAD']['channels'].replace(\"'\",'\"'))\n",
    "participant_list = json.loads(featured_config['LOAD']['participant_list'].replace(\"'\",'\"'))\n",
    "Fs = int(featured_config['LOAD']['sample_rate'])\n",
    "window_size = int(featured_config['LOAD']['window_size'])\n",
    "time = window_size/Fs\n",
    "print(f\"Fs: {Fs}, n_samples = {window_size}, time: {time}s, n_channels: {n_channels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a74c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2611d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_use = set(\n",
    "    ['Mean', 'SD', 'MAD', 'Max', 'Min',# 'SMA',\n",
    "      'Energy', 'IQR', # 'Entropy',\n",
    "     'arCoeff', 'Correlation', 'MaxFreqInd', 'MeanFreq', 'FreqSkewness',\n",
    "    'FreqKurtosis' # , 'EnergyBands'\n",
    "    ])\n",
    "columns_to_use = [ col for col in featured_df.columns if re.sub(res_digit, '', col) in features_to_use ]\n",
    "print(f\"columns_to_use = {columns_to_use}\")\n",
    "\n",
    "# designmtx = featured_df[columns_to_use].values \n",
    "# condition_data = featured_df['condition'].values.astype(int)\n",
    "# subject_data_names = featured_df['participant']\n",
    "\n",
    "# design2d = TSNE(n_components=2, init='random', perplexity=3).fit_transform(designmtx)\n",
    "# print(f\"design2d.shape = {design2d.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e283660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance featured data\n",
    "subject_condition_counts = get_group_label_counts(featured_df, 'participant', 'condition')\n",
    "print(f\"before balancing: subject_condition_counts = {subject_condition_counts}\")\n",
    "featured_df = balance_data(featured_df, group_col='participant', label_col='condition')\n",
    "subject_condition_counts = get_group_label_counts(featured_df, 'participant', 'condition')\n",
    "\n",
    "print(f\"after balancing: subject_condition_counts = {subject_condition_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b720f6e",
   "metadata": {},
   "source": [
    "## Visualising data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d167e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = np.unique(subject_data_names)\n",
    "subject_data = np.empty(subject_data_names.shape, dtype=int)\n",
    "for s, sub in enumerate(subjects):\n",
    "    subject_data[subject_data_names==sub] = s\n",
    "print(f\"designmtx.shape = {designmtx.shape}\")\n",
    "print(f\"condition_data.shape = {condition_data.shape}\")\n",
    "print(f\"subject_data.shape = {subject_data.shape}\")\n",
    "    \n",
    "conditions = np.unique(condition_data)\n",
    "markers = ['v', '^', '<', '>', 's', '*', '+' , 'x', 'D', '.']\n",
    "colours = ['b','g','r','y','k']\n",
    "cmap = plt.cm.rainbow\n",
    "norm = colors.BoundaryNorm(np.arange(np.min(subject_data)-0.5,np.max(subject_data)+0.5), cmap.N)\n",
    "\n",
    "plt.scatter(\n",
    "    design2d[:,0], design2d[:,1], c=subject_data, norm=norm, s=0.5, edgecolor='none')\n",
    "plt.colorbar(\n",
    "    ticks=np.arange(subjects.size))\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "cmap = plt.cm.rainbow\n",
    "norm = colors.BoundaryNorm(np.arange(np.min(condition_data)-0.5,np.max(condition_data)+0.5), cmap.N)\n",
    "\n",
    "plt.scatter(\n",
    "    design2d[:,0], design2d[:,1], c=condition_data, norm=norm, s=0.5, edgecolor='none')\n",
    "plt.colorbar(\n",
    "    ticks=np.arange(conditions.size))\n",
    "# for s,subject in enumerate(subjects):\n",
    "#     marker = s\n",
    "#     for c, condition in enumerate(conditions):\n",
    "#         _filter = (condition_data==condition) &(subject_data==subject)\n",
    "#         #print(f\"condition = {condition}\")\n",
    "#         #print(f\"np.sum(_filter) = {np.sum(_filter)}\")\n",
    "#         plt.plot(design2d[_filter,0], design2d[_filter,1], ls='None', marker=markers[s], color=colours[c], markeredgewidth=0.1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86e3223",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9fb7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        min_samples_leaf=5, random_state=0\n",
    "    ),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(\n",
    "        max_leaf_nodes=15, random_state=0\n",
    "    ),\n",
    "    \"MLP\":  MLPClassifier(max_iter=100)\n",
    "}\n",
    "param_grids = {\n",
    "    \"Random Forest\": {\"n_estimators\": [10, 20, 50, 100]},\n",
    "    \"Gradient Boosting\": {\"n_estimators\": [10, 20, 50, 100]},\n",
    "    \"MLP\": {\n",
    "        'hidden_layer_sizes': [(10,),(20,),(50,),(100,)],\n",
    "        'activation': ['relu'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'alpha': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05],\n",
    "        'learning_rate': ['constant','adaptive'],\n",
    "    }\n",
    "}\n",
    "\n",
    "# determine the columns to use\n",
    "features_to_use = set(\n",
    "    ['Mean', 'SD', 'MAD', 'Max', 'Min',# 'SMA',\n",
    "      'Energy', 'IQR', # 'Entropy',\n",
    "     'arCoeff', 'Correlation', 'MaxFreqInd', 'MeanFreq', 'FreqSkewness',\n",
    "    'FreqKurtosis' # , 'EnergyBands'\n",
    "    ])\n",
    "columns_to_use = [ col for col in featured_df.columns if re.sub(res_digit, '', col) in features_to_use ]\n",
    "print(f\"columns_to_use = {columns_to_use}\")\n",
    "\n",
    "designmtx = featured_df[columns_to_use].to_numpy()\n",
    "\n",
    "condition_data = featured_df['condition'].values.astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f34496d",
   "metadata": {},
   "source": [
    "## Hold one group out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b39cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard cross-validation\n",
    "# Match all digits in the string and replace them with an empty string\n",
    "# new_string = re.sub(pattern, '', string1)\n",
    "subjects = np.unique(featured_df['participant'])\n",
    "n_subjects = len(subjects)\n",
    "groups = np.empty(len(featured_df), dtype=int)\n",
    "for s, sub in enumerate(subjects):\n",
    "    groups[featured_df['participant']==sub] = s\n",
    "# cross validation splits    \n",
    "group_kfold = GroupKFold(n_splits=n_subjects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab798c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame()\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grids[name],\n",
    "        return_train_score=True,\n",
    "        cv=group_kfold,\n",
    "    ).fit(X=designmtx, y=condition_data, groups=groups)\n",
    "    result_df = pd.DataFrame(grid_search.cv_results_)\n",
    "    #test_df['model'] = result['model']\n",
    "    result_df.insert(0, 'model', name)\n",
    "    result_df.insert(1, 'held out', 'subject')\n",
    "    result_df.insert(2, 'feature set', str(features_to_use))\n",
    "    display(result_df)\n",
    "    results_df = pd.concat((results_df, result_df))\n",
    "    #result = {\"model\": name, \"cv_results\": pd.DataFrame(grid_search.cv_results_)}\n",
    "    #results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea485d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import predicament.evaluation.results\n",
    "reload(predicament.evaluation.results)\n",
    "from predicament.evaluation.results import test\n",
    "from predicament.evaluation.results import save_results_df_to_file\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941c427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results_df_to_file(results_df, 'balanced_eeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e34752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_best_from_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ef42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d15480",
   "metadata": {},
   "source": [
    "## Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15feda71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]# Create the random grid\n",
    "random_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_features': max_features,\n",
    "    'max_depth': max_depth,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db20d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator = rf,\n",
    "    param_distributions = random_grid,\n",
    "    n_iter = 100,\n",
    "    cv=group_kfold,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs = -1)\n",
    "\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X=designmtx, y=condition_data, groups=groups)\n",
    "random_result_df = pd.DataFrame(rf_random.cv_results_)\n",
    "save_results_df_to_file(random_result_df, 'random_search_random_forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff5afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian optimisation\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "# parameter ranges are specified by one of below\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X, y = load_iris(True)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "#      train_size=0.75,\n",
    "#     random_state=0)\n",
    "\n",
    "# estimator = SVC()\n",
    "# search_spaces =      {\n",
    "#          'C': Real(1e-6, 1e+6, prior='log-uniform'),\n",
    "#          'gamma': Real(1e-6, 1e+1, prior='log-uniform'),\n",
    "#          'degree': Integer(1,8),\n",
    "#          'kernel': Categorical(['linear', 'poly', 'rbf']),\n",
    "#      }\n",
    "estimator = MLPClassifier(max_iter=100)\n",
    "#     \"MLP\": {\n",
    "#         'hidden_layer_sizes': [(10,),(20,),(50,),(100,)],\n",
    "#         'activation': ['relu'],\n",
    "#         'solver': ['sgd', 'adam'],\n",
    "#         'alpha': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05],\n",
    "#         'learning_rate': ['constant','adaptive'],\n",
    "#     }\n",
    "search_spaces =      {\n",
    "        'hidden_layer_sizes': Categorical([ (n,) for n in range(10,200,10)]),\n",
    "        'activation': Categorical(['tanh', 'relu']),\n",
    "        'solver': Categorical(['sgd', 'adam']),\n",
    "        'alpha': Real(1e-6, 1e+1, prior='log-uniform'),\n",
    "        'learning_rate': Categorical(['constant','adaptive']),\n",
    "     }\n",
    "\n",
    "# log-uniform: understand as search over p = exp(x) by varying x\n",
    "param_search = BayesSearchCV(\n",
    "    estimator, search_spaces,\n",
    "    cv=group_kfold, verbose=2, random_state=42, n_iter=50)\n",
    "\n",
    "# executes bayesian optimization\n",
    "_ = param_search.fit(X=designmtx, y=condition_data, groups=groups)\n",
    "result_df = pd.DataFrame(param_search.cv_results_)\n",
    "result_df.insert(0, 'model', name)\n",
    "result_df.insert(1, 'held out', 'subject')\n",
    "result_df.insert(2, 'feature set', str(features_to_use))\n",
    "display(result_df)\n",
    "# results_df = pd.concat((results_df, result_df))\n",
    "\n",
    "save_results_df_to_file(result_df, 'bayes_search')\n",
    "\n",
    "# # model can be saved, used for predictions or scoring\n",
    "# print(opt.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621bcd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(result_df[result_df['mean_test_score'] == result_df['mean_test_score'].max()].loc[37])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ac6d1c",
   "metadata": {},
   "source": [
    "# Balanced data \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27102ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = np.unique(balanced_featured_df['participant'])\n",
    "n_subjects = len(subjects)\n",
    "groups = np.empty(len(balanced_featured_df), dtype=int)\n",
    "for s, sub in enumerate(subjects):\n",
    "    groups[balanced_featured_df['participant']==sub] = s\n",
    "    \n",
    "features_to_use = set(\n",
    "    ['Mean', 'SD', 'MAD', 'Max', 'Min',# 'SMA',\n",
    "      'Energy', 'IQR', # 'Entropy',\n",
    "     'arCoeff', 'Correlation', 'MaxFreqInd', 'MeanFreq', 'FreqSkewness',\n",
    "    'FreqKurtosis' # , 'EnergyBands'\n",
    "    ])\n",
    "columns_to_use = [ col for col in balanced_featured_df.columns if re.sub(res_digit, '', col) in features_to_use ]\n",
    "print(f\"columns_to_use = {columns_to_use}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdedeb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "designmtx = balanced_featured_df[columns_to_use].values \n",
    "condition_data = balanced_featured_df['condition'].values.astype(int)\n",
    "\n",
    "balanced_results_df = pd.DataFrame()\n",
    "group_kfold = GroupKFold(n_splits=n_subjects)\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grids[name],\n",
    "        return_train_score=True,\n",
    "        cv=group_kfold,\n",
    "    ).fit(X=designmtx, y=condition_data, groups=groups)\n",
    "    result_df = pd.DataFrame(grid_search.cv_results_)\n",
    "    #test_df['model'] = result['model']\n",
    "    result_df.insert(0, 'model', name)\n",
    "    result_df.insert(1, 'held out', 'subject')\n",
    "    result_df.insert(2, 'feature set', str(features_to_use))\n",
    "    display(result_df)\n",
    "    balanced_results_df = pd.concat((balanced_results_df, result_df))\n",
    "    #result = {\"model\": name, \"cv_results\": pd.DataFrame(grid_search.cv_results_)}\n",
    "    #results.append(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
