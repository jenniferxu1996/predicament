{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ab621dc",
   "metadata": {},
   "source": [
    "This is a live notebook with experimental code to develop analysis investigating the correlation between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "518e95ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['debug_windowing_data.ipynb', 'testing_mlpwrapper.ipynb', 'logs', 'analyse_results.ipynb', 'gait_data_features.ipynb', 'feature_development.ipynb', 'data', 'feature_analysis.ipynb', 'test_sample_entropy_implementations.ipynb', 'merging_dataframes.ipynb', 'data_and_processing_description.ipynb', 'featured_prediction_random_forest.ipynb', 'window_timings_and_IBI.ipynb', 'gait_data_exploration.ipynb', 'processing_status.ipynb', '.ipynb_checkpoints', 'loading_e4_data.ipynb', 'data_investigation_scratch.ipynb', 'simply_load_and_inspect_data.ipynb', 'grouping_and_crossvalidation.ipynb', 'sample-entropy-numba-impl.ipynb', 'featured_prediction.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import configparser\n",
    "\n",
    "import os\n",
    "print(os.listdir(\".\"))\n",
    "\n",
    "# import scipy.stats as spstats\n",
    "# # fourier transform\n",
    "# from scipy.fft import fft, ifft\n",
    "\n",
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# # from statsmodels.tsa.api import acf, graphics, pacf\n",
    "# from statsmodels.tsa.ar_model import AutoReg\n",
    "# # from statsmodels.tsa.ar_model import ar_select_order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9652555c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thisdir = /home/luke/git/external/predicament/notebooks\n",
      "Adding parent directory to python path\n",
      "sys.path =\n",
      "['/home/luke/git/external/predicament/notebooks', '/home/luke/git/external/predicament', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/home/luke/.local/lib/python3.10/site-packages', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/lib/python3.10/dist-packages']\n"
     ]
    }
   ],
   "source": [
    "# This is a hack to make the library in the parent folder available for imoprts\n",
    "# A better solution is by np8 here:\n",
    "# https://stackoverflow.com/questions/714063/importing-modules-from-parent-folder\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "thisdir = sys.path[0]\n",
    "print(f\"thisdir = {thisdir}\")\n",
    "parentdir = os.path.dirname(thisdir)\n",
    "#print(f\"parentdir = {parentdir}\")\n",
    "if not parentdir in sys.path:\n",
    "    print(\"Adding parent directory to python path\")\n",
    "    sys.path.insert(1, parentdir)\n",
    "else:\n",
    "    print(\"Skipping adding parent direct to path (there already)\")\n",
    "\n",
    "print(f\"sys.path =\\n{sys.path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28f17581",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ensure relative path to data directory is sound\n",
    "# for the notebook we need to modify the BASE_DATA_FOLDER\n",
    "import os \n",
    "os.environ['PREDICAMENT_DATA_DIR'] =  '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd724667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predicament.utils.config import DREEM_EEG_CHANNELS\n",
    "from predicament.utils.config import FEATURED_BASE_PATH\n",
    "from predicament.utils.config import WINDOWED_BASE_PATH\n",
    "from predicament.utils.file_utils import drop_nan_cols\n",
    "from predicament.utils.file_utils import load_dataframe_and_config\n",
    "from predicament.utils.file_utils import write_dataframe_and_config\n",
    "from predicament.utils.config_parser import config_to_dict\n",
    "\n",
    "from predicament.data.timeseries import create_participant_data_edf_only\n",
    "from predicament.data.windowed import window_all_participants_data\n",
    "from predicament.data.windowed import merge_condition_data\n",
    "from predicament.data.partitioning import between_subject_cv_partition\n",
    "\n",
    "from predicament.data.features import IDEAL_FEATURE_GROUP\n",
    "from predicament.data.features import STATS_FEATURE_GROUP\n",
    "from predicament.data.features import INFO_FEATURE_GROUP\n",
    "from predicament.data.features import FREQ_FEATURE_GROUP\n",
    "from predicament.data.features import convert_timeseries_to_features\n",
    "from predicament.data.features import filter_features\n",
    "from predicament.data.features import derive_feature_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3596cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'VG_01': '../data/CARE_HOME_DATA/./VG01/E4_8921_15_44/',\n",
       " 'VG_03': '../data/CARE_HOME_DATA/./VG03/E4_9921_12_16/',\n",
       " 'VG_05': '../data/CARE_HOME_DATA/./VG05/E4_9921_13_24/',\n",
       " 'VG_06': '../data/CARE_HOME_DATA/./VG06/E4_51021_13_33/',\n",
       " 'VG_07': '../data/CARE_HOME_DATA/./VG07/E4_51021_15_39/',\n",
       " 'VG_08': '../data/CARE_HOME_DATA/./VG08/E4_71021_10_42/',\n",
       " 'VG_09': '../data/CARE_HOME_DATA/./VG09/E4_11221_14_46/',\n",
       " 'VG_10': '../data/CARE_HOME_DATA/./VG10/E4_31221_11_17/',\n",
       " 'VH_01': '../data/CARE_HOME_DATA/./VH01/E4_61021_11_03/',\n",
       " 'VH_02': '../data/CARE_HOME_DATA/./VH02/E4_61021_13_59/',\n",
       " 'VH_03': '../data/CARE_HOME_DATA/./VH03/E4_11221_11_22/'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from predicament.utils.config import E4_CSV_FILES\n",
    "from predicament.utils.config import E4_FULL_DIRPATHS\n",
    "E4_FULL_DIRPATHS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3230ea6",
   "metadata": {},
   "source": [
    "## dreem (EEG) data or E4 data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77ccb2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subdir = dreem_4secs\n",
      "non-ideal features: {'LempelZivEntropy', 'Energy', 'FreqSkewness'}\n",
      "missing ideal features: {'HRVRMSSD'}\n",
      "\n",
      "subdir = dreem_10secs\n",
      "non-ideal features: {'LempelZivEntropy', 'Energy', 'FreqSkewness'}\n",
      "missing ideal features: {'HRVRMSSD'}\n",
      "\n",
      "subdir = E4_4secs\n",
      "non-ideal features: {'LempelZivEntropy', 'FreqSkewness'}\n",
      "missing ideal features: set()\n",
      "\n",
      "subdir = E4_10secs\n",
      "non-ideal features: {'LempelZivEntropy', 'FreqSkewness'}\n",
      "missing ideal features: set()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subdirs = ['dreem_4secs', 'dreem_10secs', 'E4_4secs', 'E4_10secs']\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "subdir_feature_types = {}\n",
    "subdir_label_mappings = {}\n",
    "combined_types = set()\n",
    "combined_conditions = set()\n",
    "\n",
    "for subdir in subdirs: \n",
    "    print(f\"subdir = {subdir}\")\n",
    "    data_dir = os.path.join(FEATURED_BASE_PATH,subdir)\n",
    "    config_path = os.path.join(data_dir, 'details.cfg')\n",
    "    with open(config_path, 'r') as config_file:\n",
    "        config.read_file(config_file)\n",
    "# convert config to dictionary\n",
    "    dict_config =  config_to_dict(config)\n",
    "    feature_names = dict_config['FEATURED']['feature_names']\n",
    "    feature_set = dict_config['FEATURED']['feature_set']\n",
    "    label_cols = dict_config['WINDOWED']['label_cols']\n",
    "    feature_types = derive_feature_types(feature_names, feature_set, label_cols)\n",
    "    print(f\"non-ideal features: {set(feature_set)-IDEAL_FEATURE_GROUP}\")\n",
    "    print(f\"missing ideal features: {IDEAL_FEATURE_GROUP-set(feature_set)}\")\n",
    "    subdir_feature_types[subdir] = feature_types\n",
    "    combined_types.update(feature_types)\n",
    "    label_mapping = dict_config['LOAD']['label_mapping']\n",
    "    subdir_label_mappings[subdir] = label_mapping\n",
    "    combined_conditions.update(label_mapping)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a02ebc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing each subdir with combined features from all:\n",
      "\n",
      "\n",
      "Combined is Mean;Hurst_H;LempelZivEntropy[b=1];IQR;Max;Min;LempelZivComplexity[b=8];Hurst_C;MeanFreq;MAD;LempelZivComplexity[b=4];SD;FreqSkewness;SampleEntropy[m=2];arCoeff;Correlation;LyapunovExponent;HRVRMSSD[Normed];LempelZivEntropy[b=4];FreqKurtosis;LempelZivComplexity[b=2];Energy;MaxFreqInd;LempelZivComplexity[b=1]\n",
      "\n",
      "\n",
      "dreem_4secs is missing:\n",
      "\t{'HRVRMSSD[Normed]'}\n",
      "dreem_10secs is missing:\n",
      "\t{'HRVRMSSD[Normed]', 'LempelZivEntropy[b=4]'}\n",
      "E4_4secs is missing:\n",
      "\t{'Energy', 'LempelZivEntropy[b=4]'}\n",
      "E4_10secs is missing:\n",
      "\t{'Energy', 'LempelZivEntropy[b=4]'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Comparing each subdir with combined features from all:\\n\\n\")\n",
    "print(f\"Combined is {';'.join(combined_types)}\\n\\n\")\n",
    "for subdir in subdirs:\n",
    "    diff = combined_types - set(subdir_feature_types[subdir])\n",
    "    print(f\"{subdir} is missing:\\n\\t{diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38b96752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing each subdir with combined conditions from all:\n",
      "Combined is {'familiar_music', 'tchaikovsky', 'family_inter', 'wildlife_video', 'exper_video'}\n",
      "dreem_4secs is missing:\n",
      "\tset()\n",
      "dreem_10secs is missing:\n",
      "\tset()\n",
      "E4_4secs is missing:\n",
      "\tset()\n",
      "E4_10secs is missing:\n",
      "\tset()\n"
     ]
    }
   ],
   "source": [
    "print(\"Comparing each subdir with combined conditions from all:\")\n",
    "print(f\"Combined is {combined_conditions}\")\n",
    "for subdir in subdirs:\n",
    "    diff = combined_conditions - set(subdir_label_mappings[subdir])\n",
    "    print(f\"{subdir} is missing:\\n\\t{diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e14d13e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
